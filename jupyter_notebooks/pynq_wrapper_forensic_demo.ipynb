{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization and some auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%pybind11/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/share/pynq-venv/lib/python3.8/site-packages/pynq/overlay.py:681: UserWarning: Interrupt s2mm_introut not created: Could not find UIO device for interrupt pin for IRQ number 62\n",
      "  warnings.warn('Interrupt {} not created: {}'.format(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continuous_monitoring_system_controller config loaded:\n",
      "{'atf_active': True,\n",
      " 'atf_mode': 1,\n",
      " 'basic_trace_filter_mode': 1,\n",
      " 'basic_trace_filter_time_interval_ticks': 1000,\n",
      " 'basic_trace_filter_time_interval_type': 1,\n",
      " 'external_trace_filter_mode_enabled': False,\n",
      " 'feature_extractor_halting_cpu_enabled': False,\n",
      " 'halting_cpu': False,\n",
      " 'monitored_address_range_lower_bound': 4095,\n",
      " 'monitored_address_range_lower_bound_enabled': False,\n",
      " 'monitored_address_range_upper_bound': 2147483903,\n",
      " 'monitored_address_range_upper_bound_enabled': False,\n",
      " 'tlast_interval': 0,\n",
      " 'trigger_trace_end_address': 2147483910,\n",
      " 'trigger_trace_end_address_enabled': False,\n",
      " 'trigger_trace_start_address': 4096,\n",
      " 'trigger_trace_start_address_enabled': False}\n",
      "buffer_length = 6250000\n",
      "Performance events count = 8\n",
      "\n",
      "Initialization done\n",
      "\n",
      "Operational_Config({'disable_saving': False,\n",
      " 'f_name': 'config.pickle',\n",
      " 'items_collected_processing_limit': 500,\n",
      " 'lack_of_matches_threshold_multiplier_of_max_interval': 1.5,\n",
      " 'periodic_send_interval_seconds': 0.8,\n",
      " 'raw_data_send_enable': True})\n"
     ]
    }
   ],
   "source": [
    "overlay_fname = \"imported_design.bit\"\n",
    "#overlay_fname = \"imported_design_cheribsd.bit\"\n",
    "\n",
    "#import ipdb # alternative to pdb that works in jupyter notebook (pip3 install ipdb)\n",
    "from IPython.core.debugger import set_trace\n",
    "import os, subprocess, sys, re, time, inspect, logging, random, json, math, glob, datetime\n",
    "from pathlib import Path\n",
    "from pynq import Overlay, allocate\n",
    "#from pynq import GPIO\n",
    "from threading import Thread, Lock\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG) # logging.INFO)\n",
    "\n",
    "pynq_restarted = True\n",
    "\n",
    "#BENCHMARK_TOOL_DIR = Path('/home/xilinx/benchmark_files/scripts/')\n",
    "#sys.path.append(str(BENCHMARK_TOOL_DIR))\n",
    "#import compare_classification_methods_2 as benchmark_ccm\n",
    "\n",
    "PROGRAMS_DIR = Path('/home/xilinx/programs')\n",
    "\n",
    "from dma_receiver import DmaReceiver\n",
    "from bram_loader import Bram_Loader\n",
    "from continuous_monitoring_system_controller import ContinuousMonitoringSystemController, BASIC_TRACE_FILTER_MODE\n",
    "from riscv_instruction_decoder import get_riscv_instruction_name\n",
    "from tcp_server import TCP_Server, get_my_ip\n",
    "from console_io import Console_IO\n",
    "import advanced_trace_filter\n",
    "from anomaly_detection import Anomaly_Detection\n",
    "from advanced_trace_filter import ATF_Watchpoints, ATF_MODE\n",
    "from parse_objdump import parse_objdump\n",
    "from packet_format import Packet_Format, DataFrame_Columns_Order\n",
    "from sql_db import SQL_Barcodes_DB\n",
    "from operational_config import Operational_Config\n",
    "\n",
    "operational_config = Operational_Config('config.pickle')\n",
    "operational_config.load()\n",
    "\n",
    "TCP_SERVER_PORT = 9093\n",
    "# tcp server for communicating with display (e.g. ESP3248S035C, but really any TCP client that connects)\n",
    "tcp_server = TCP_Server(host_ip='0.0.0.0', port=TCP_SERVER_PORT)\n",
    "\n",
    "BASE_DIR = Path('/home/xilinx/design_files')\n",
    "OUTPUT_DIR = Path('/home/xilinx/output_files')\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.mkdir(OUTPUT_DIR)\n",
    "\n",
    "base = Overlay(str(BASE_DIR / overlay_fname))\n",
    "\n",
    "bram_loader = Bram_Loader(base.PYNQ_wrapper_blocks.bram_loader.axi_gpio_2)\n",
    "console_io = Console_IO(\n",
    "    base.PYNQ_wrapper_blocks.console_io.axi_dma_console_io,\n",
    "    recv_buffer_capacity=10000,\n",
    "    send_buffer_capacity=10000\n",
    "    )\n",
    "\n",
    "# the long name is because of using hierarchy in Vivado block design\n",
    "cms_ctrl_axi_gpio = base.PYNQ_wrapper_blocks.continuous_monitoring_system_blocks.axi_gpio_to_cms_ctrl_interface.axi_gpio_cms_ctrl.channel1    \n",
    "cms_ctrl = ContinuousMonitoringSystemController(cms_ctrl_axi_gpio, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "INPUT_BUFFER_DTYPE_SIZE_IN_BYTES = 8\n",
    "#FIFO_SIZE = 32768\n",
    "# +4 because DMA seems to have it's own buffer it fills before dma.recvchannel.transfer is even called\n",
    "#buffer_length = min( base.PYNQ_wrapper_blocks.continuous_monitoring_system_blocks.axi_dma_0.buffer_max_size // ITEM_BYTE_SIZE, FIFO_SIZE)# + 4) \n",
    "#buffer_length = 4_000_000 // 8 # 4MB in total\n",
    "#buffer_length = 16_000_000 // 8 # 16MB in total\n",
    "#buffer_length = 10240*10 // 8 \n",
    "buffer_length = 50_000_000 // INPUT_BUFFER_DTYPE_SIZE_IN_BYTES # 50MB in total\n",
    "print('buffer_length =', buffer_length)\n",
    "\n",
    "input_buffer = allocate(shape=(buffer_length,), dtype='u8')\n",
    "input_buffer_2 = allocate(shape=(buffer_length,), dtype='u8')\n",
    "\n",
    "dma_rec = base.PYNQ_wrapper_blocks.continuous_monitoring_system_blocks.axi_dma_0.recvchannel\n",
    "\n",
    "# https://pynq.readthedocs.io/en/v2.7.0/_modules/pynq/lib/axigpio.html\n",
    "gpio_rst_n_out = base.PYNQ_wrapper_blocks.axi_gpio_0.channel1[0]\n",
    "gpio_rst_n_console_input = base.PYNQ_wrapper_blocks.axi_gpio_0.channel1[1]\n",
    "gpio_rst_n_console_output = base.PYNQ_wrapper_blocks.axi_gpio_0.channel1[2]\n",
    "#gpio_en_cpu_reset_server_request_put_out = base.axi_gpio_0.channel1[1]\n",
    "#gpio_pc_stream_m_axis_tlast_interval = base.axi_gpio_1.channel1\n",
    "\n",
    "gpio_fifo_wr_count = base.PYNQ_wrapper_blocks.axi_gpio_0.channel2[0:16]\n",
    "gpio_fifo_rd_count = base.PYNQ_wrapper_blocks.axi_gpio_0.channel2[16:32]\n",
    "\n",
    "# PERFORMANCE_EVENTS_FNAME = 'performance_event_names_selected.csv'\n",
    "PERFORMANCE_EVENTS_FNAME = 'performance_event_names_used.csv'\n",
    "with open(PERFORMANCE_EVENTS_FNAME) as f:    \n",
    "    PERFORMANCE_EVENTS_COUNT = len(f.readlines()) - 1\n",
    "print(f'Performance events count = {PERFORMANCE_EVENTS_COUNT}')\n",
    "# PERFORMANCE_COUNTER_WIDTH = 7\n",
    "# PERFORMANCE_COUNTERS_OVERFLOW_MAP_WIDTH = PERFORMANCE_EVENTS_COUNT\n",
    "PERFORMANCE_COUNTER_WIDTH = 32\n",
    "PERFORMANCE_COUNTERS_OVERFLOW_MAP_WIDTH = PERFORMANCE_EVENTS_COUNT\n",
    "PC_WIDTH = 64\n",
    "INSTR_WIDTH = 32\n",
    "CLK_COUNTER_WIDTH = 64\n",
    "FIFO_FULL_TICKS_COUNT_WIDTH = 64\n",
    "GP_REGISTER_WIDTH = 128\n",
    "AXI_DATA_WIDTH = 1024\n",
    "FEATURE_EXTRACTOR_RESULT_WIDTH = 40\n",
    "USED_AXI_BITS = sum([\n",
    "    PERFORMANCE_EVENTS_COUNT*PERFORMANCE_COUNTER_WIDTH,\n",
    "    PERFORMANCE_EVENTS_COUNT,\n",
    "    PC_WIDTH,\n",
    "    CLK_COUNTER_WIDTH,\n",
    "    FIFO_FULL_TICKS_COUNT_WIDTH,\n",
    "    INSTR_WIDTH,\n",
    "    4*64, # A0 - A3\n",
    "    FEATURE_EXTRACTOR_RESULT_WIDTH\n",
    "    ])\n",
    "CLK_SPEED = 50_000_000\n",
    "# how many items from AXI can be stored in PYNQ allocated buffer\n",
    "BUFFER_ITEM_CAPACITY = buffer_length // AXI_DATA_WIDTH * 8 # bytes / bits * bits_per_byte\n",
    "\n",
    "# input buffer has \"u8\" dtype which has 8 bytes per element\n",
    "# 16 elements are needed to store a single 1024-bit item from FIFO\n",
    "# variable below can be used to know location of the end of transferred data in the input buffer\n",
    "# so we can copy it and initiate another transfer\n",
    "INPUT_BUFFER_LOCATIONS_PER_ITEM = AXI_DATA_WIDTH / 8 / INPUT_BUFFER_DTYPE_SIZE_IN_BYTES\n",
    "\n",
    "# theoretically with 16MB allocated and 1024-bit items we could set TLAST_INTERVAL to 125000\n",
    "#TLAST_INTERVAL = BUFFER_ITEM_CAPACITY - 5000\n",
    "TLAST_INTERVAL = 0 # axilite_tap based tlast (setting tlast when receive transfer is requested)\n",
    "\n",
    "def print_dma_channel_status(channel):\n",
    "    print('dma.running =', channel.running)\n",
    "    print('dma.idle =', channel.idle)\n",
    "    print('dma.error =', channel.error)\n",
    "    print('status =', hex(channel._mmio.read(channel._offset + 4)))\n",
    "    \n",
    "def reset_cpu(delay=0.001):\n",
    "    ''' AXI GPIO controlled reset, active-low. '''\n",
    "    #gpio_en_cpu_reset_server_request_put_out.write(0)\n",
    "    gpio_rst_n_out.write(0)\n",
    "    time.sleep(delay)\n",
    "    gpio_rst_n_out.write(1)\n",
    "    time.sleep(delay)\n",
    "    #gpio_en_cpu_reset_server_request_put_out.write(1)\n",
    "    #time.sleep(delay)\n",
    "    #gpio_en_cpu_reset_server_request_put_out.write(0)\n",
    "    #time.sleep(delay)\n",
    "    \n",
    "def print_fifo_data_counts():\n",
    "    print('gpio_fifo_wr_count =', gpio_fifo_wr_count.read())\n",
    "    print('gpio_fifo_rd_count =', gpio_fifo_rd_count.read())\n",
    "    \n",
    "def instr_to_strings(instructions_integers):\n",
    "    ''' Requires riscv-python-model installed.\n",
    "    If network connection is available, \"python3 -m pip install riscv-model.\n",
    "    If not, then on separate machine with internet:\n",
    "        python3 -m pip download riscv-model -d .  \n",
    "    Then copy the downloaded .whl file to pynq and install with:\n",
    "        python3 -m pip install <file.whl> -f ./ --no-index   \n",
    "    Usage:\n",
    "        instr_to_string([0xB60006F, 0xFE0791E3])\n",
    "        '''\n",
    "    instructions_string = ' 0x'.join(f'{ii:08X}' for ii in instructions_integers)\n",
    "    return os.popen(f'riscv-machinsn-decode hexstring {instructions_string}').read().strip().split('\\n')\n",
    "\n",
    "\n",
    "####################################################################\n",
    "# \n",
    "\n",
    "def read_performance_event_names(f_name):\n",
    "    ''' Reads events names from file, these were collected from CHERI-Flute source code by using this script:\n",
    "    https://github.com/michalmonday/Flute/blob/continuous_monitoring/builds/RV64ACDFIMSUxCHERI_Flute_verilator/vcd/read_vcd.py\n",
    "    '''\n",
    "    with open(f_name) as f:\n",
    "        return [line.strip().split(',')[2] for line in f.readlines()[1:]]\n",
    "\n",
    "def pop_n_bits_value(val, n):\n",
    "    ''' pop_n_bits_value(0xFFFF, 4) returns tuple like: (0xFFF, 0xF) '''\n",
    "    bits_value = val & ((1<<n)-1)\n",
    "    return val >> n, bits_value\n",
    "\n",
    "# def parse_fifo_item(fifo_item):\n",
    "#     ''' Parses a single fifo item (e.g. 1024 bits) numerical value. \n",
    "#         Single fifo item = {59bits padding, performance_counters805(7bits*115counters), instr32, clk_counter_delta64, pc64}\n",
    "#         Padding is used because only power of 2s can be used as size in fifo generator block (or axi in general?)'''\n",
    "#     perf_counters = []\n",
    "#     for i in range(PERFORMANCE_EVENTS_COUNT):\n",
    "#         fifo_item, perf_counter = pop_n_bits_value(fifo_item, PERFORMANCE_COUNTER_WIDTH)\n",
    "#         perf_counters.append(perf_counter)\n",
    "#     fifo_item, perf_counters_overflow_map = pop_n_bits_value(fifo_item, PERFORMANCE_COUNTERS_OVERFLOW_MAP_WIDTH)\n",
    "#     fifo_item, pc = pop_n_bits_value(fifo_item, PC_WIDTH)\n",
    "#     fifo_item, clk_counter = pop_n_bits_value(fifo_item, CLK_COUNTER_WIDTH)\n",
    "#     fifo_item, instr = pop_n_bits_value(fifo_item, INSTR_WIDTH)\n",
    "#     fifo_item, fifo_full_ticks_count = pop_n_bits_value(fifo_item, FIFO_FULL_TICKS_COUNT_WIDTH)\n",
    "#     fifo_item, gp_reg_A0 = pop_n_bits_value(fifo_item, 64)\n",
    "#     fifo_item, gp_reg_A1 = pop_n_bits_value(fifo_item, 64)\n",
    "#     fifo_item, gp_reg_A2 = pop_n_bits_value(fifo_item, 64)\n",
    "#     fifo_item, gp_reg_A3 = pop_n_bits_value(fifo_item, 64)\n",
    "#     gp_regs = {'A0':gp_reg_A0, 'A1':gp_reg_A1, 'A2':gp_reg_A2, 'A3':gp_reg_A3}\n",
    "#     return perf_counters, perf_counters_overflow_map, pc, clk_counter, instr, fifo_full_ticks_count, gp_regs\n",
    "\n",
    "def parse_fifo_item(fifo_item, packet_format):\n",
    "    ''' Parses a single fifo item (e.g. 1024 bits) numerical value. \n",
    "        Single fifo item = {59bits padding, performance_counters805(7bits*115counters), instr32, clk_counter_delta64, pc64}\n",
    "        Padding is used because only power of 2s can be used as size in fifo generator block (or axi in general?)'''\n",
    "    metrics_dict = {}\n",
    "    for metric_name, bit_width in packet_format.items():\n",
    "        fifo_item, metric_value = pop_n_bits_value(fifo_item, bit_width)\n",
    "        metrics_dict[metric_name] = metric_value\n",
    "    return metrics_dict\n",
    "\n",
    "def get_dma_transfer(input_buffer, dma_rec=dma_rec, dont_wait=False, timeout_ms=None):\n",
    "    ''' Returns the number of transferred items, each having 1024 bits. \n",
    "    This function relies on the hardware implementation that always delivers additional\n",
    "    item to indicate end of transfer. (this allows to avoid situation where dma transfer\n",
    "    hangs due to empty FIFO, that is why theres \"-1\" in return) \n",
    "    \n",
    "    timeout_ms allows to repeat transfer until some value is received '''\n",
    "    \n",
    "    def get_dma_transfer_internal():\n",
    "        ''' Helper function to avoid code repetition for repetitive transfers\n",
    "        where timeout is used. items_transferred returned by it includes the ending item.\n",
    "        So if FIFO is empty, this function returns 1. '''\n",
    "        #print(\"get_dma_transfer_internal starting new transfer\")\n",
    "        dma_rec.transfer(input_buffer)\n",
    "        if dont_wait:\n",
    "            return 1\n",
    "        #print(\"get_dma_transfer_internal waiting\")\n",
    "        dma_rec.wait()\n",
    "        #print(\"get_dma_transfer_internal calculating transferred items\")\n",
    "        items_transferred = math.floor(dma_rec.transferred * 64 / AXI_DATA_WIDTH / 8)\n",
    "        #print(\"get_dma_transfer_internal returning\")\n",
    "        return items_transferred\n",
    "        \n",
    "    items_transferred = 1\n",
    "    # repetitive transfers when timeout_ms is used\n",
    "    if timeout_ms is not None:\n",
    "        timeout_s = timeout_ms / 1000\n",
    "        start_time = time.time()\n",
    "        while items_transferred <= 1 and time.time() - start_time < timeout_s:\n",
    "            items_transferred = get_dma_transfer_internal()\n",
    "            time.sleep(0.01)\n",
    "        return items_transferred - 1\n",
    "    \n",
    "    # single transfer when timeout_ms is not used\n",
    "    items_transferred = get_dma_transfer_internal()\n",
    "    #print(f'items_transferred = {items_transferred}')\n",
    "    return items_transferred - 1\n",
    "\n",
    "def parse_input_buffer(input_buffer, items_transferred, packet_format=Packet_Format.data_pkt):\n",
    "    ''' Function that parses the DMA receive buffer and returns a pandas DataFrame of all features like:\n",
    "    - hardware performance events (with their overflow map)\n",
    "    - program counters\n",
    "    - number of clock ticks beween received items\n",
    "    - instructions (in numerical form)\n",
    "    - instructions decoded (in string form), this is slow so \"dont_decode=True\" may be used\n",
    "    - number of clock ticks while internal trace storage was full and CPU was halted for that reason\n",
    "      (allowing to measure performace decrease due to the use of this Continuous Monitoring System)\n",
    "    - general purpose registers\n",
    "    '''\n",
    "    chunks_per_item = math.ceil(AXI_DATA_WIDTH/64)\n",
    "    start = 0\n",
    "    end = chunks_per_item\n",
    "    #time_checkpoint = time.time()\n",
    "    all_metrics = []\n",
    "    for i in range(items_transferred):\n",
    "        if i != 0:\n",
    "            start += chunks_per_item\n",
    "            end += chunks_per_item\n",
    "        #time_checkpoint = time.time()\n",
    "        fifo_item = int.from_bytes(bytes(input_buffer[start:end]), byteorder='little')\n",
    "        #print(f'{time.time() - time_checkpoint}s')\n",
    "        metrics_dict = parse_fifo_item(fifo_item, packet_format)\n",
    "                                                    \n",
    "        all_metrics.append(metrics_dict)\n",
    "        \n",
    "    df_metrics = pd.DataFrame(all_metrics)\n",
    "    return df_metrics\n",
    "\n",
    "def preprocess_df_metrics(df, dont_decode=False):\n",
    "    '''df is the DataFrame returned by \"parse_input_buffer\"    \n",
    "    Depending on available columns, this function may add some more columns like:\n",
    "    - instr_names                     (dependent on 'instr')\n",
    "    - instr_strings                   (dependent on 'instr')\n",
    "    - clk_counter_halt_agnostic       (dependent on 'clk_counter' and 'fifo_full_ticks_count')\n",
    "    - total_clk_counter_halt_agnostic (dependent on 'clk_counter_halt_agnostic')    '''\n",
    "    if 'pc' in df.columns:\n",
    "        # convert to hex string\n",
    "        df['pc'] = df['pc'].apply(lambda x: f'{x:8X}')\n",
    "    \n",
    "    if 'instr' in df.columns:\n",
    "        df['instr_names'] = df['instr'].apply(get_riscv_instruction_name)\n",
    "        # df['instr_strings'] = '-' if dont_decode else instr_to_strings(df['instr'])\n",
    "        # set all instr_strings to '-' if dont_decode=True\n",
    "        instrs = df['instr']\n",
    "        df['instr_strings'] = ['-'] * len(instrs) if dont_decode else instr_to_strings(instrs)\n",
    "        df['instr'] = df['instr'].apply(lambda x: f'{x:08X}')\n",
    "\n",
    "    if 'HPC_overflow_map' in df.columns:\n",
    "        # convert to binary string with 39 ones and zeros\n",
    "        # (it is assumed here that HPC overflow map has 39 bits, one for each HPC)\n",
    "        df['HPC_overflow_map'] = df['HPC_overflow_map'].apply(lambda x: f'{x:08b}')\n",
    "\n",
    "    if 'HPC_event_map' in df.columns:\n",
    "        df['HPC_event_map'] = df['HPC_event_map'].apply(lambda x: f'{x:08b}')\n",
    "        \n",
    "    if 'clk_counter' in df.columns and 'fifo_full_ticks_count' in df.columns:\n",
    "        df['clk_counter_halt_agnostic'] = df['clk_counter'] - df['fifo_full_ticks_count']   \n",
    "\n",
    "    return df\n",
    "\n",
    "def postprocess_df_metrics(df, columns_order=DataFrame_Columns_Order.data_pkt_columns):\n",
    "    ''' This function can only be called after the whole dataframe was collected. '''\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"WARNING: Postprocessing can't be done because df is empty, returning empty df.\")\n",
    "        return df\n",
    "    \n",
    "    for col in ['clk_counter', 'fifo_full_ticks_count', 'clk_counter_halt_agnostic']:\n",
    "        if col in df.columns:\n",
    "            df.loc[0, col] = 0\n",
    "            df[f'total_{col}'] = df[col].cumsum()     \n",
    "    df = reorder_df_columns(df, columns_order=columns_order)\n",
    "    return df\n",
    "\n",
    "input_buffer_all_transfers_copied = []\n",
    "\n",
    "\n",
    "def collect_program_data(input_buffer, dont_decode=False, dont_wait=False, dont_parse=False, copy_collected=False, \n",
    "                         execution_time_limit=None, packet_format=Packet_Format.data_pkt, \n",
    "                         columns_order=DataFrame_Columns_Order.data_pkt_columns, debug=False):\n",
    "    global dma_rec, input_buffer_all_transfers_copied, BUFFER_ITEM_CAPACITY\n",
    "    \n",
    "    if (execution_time_limit is not None) and dont_parse:\n",
    "        raise Exception(\"Execution time can't be checked without parsing received data\")\n",
    "\n",
    "    i = 0\n",
    "    total_items = 0\n",
    "    input_buffer_all_transfers_copied = []\n",
    "    df_metrics = pd.DataFrame()\n",
    "    total_execution_clocks = 0\n",
    "    while True:\n",
    "        # transfer all collected data\n",
    "        if debug:\n",
    "            print(f'Initiating DMA transfer i={i}')\n",
    "        items_transferred = get_dma_transfer(input_buffer, dma_rec, timeout_ms=1000)#, dont_wait=True) \n",
    "        if items_transferred < 1:\n",
    "            if debug:\n",
    "                print(\"NO ITEMS TRANSFERRED, PROGRAM LIKELY FINISHED\")\n",
    "            break\n",
    "        i += 1\n",
    "        \n",
    "        total_items += items_transferred\n",
    "        if dont_parse:\n",
    "            if debug:\n",
    "                print(f'Transfer {i} finished (not parsing), items_transferred={items_transferred}.')\n",
    "            if copy_collected:\n",
    "                if debug:\n",
    "                    print(f'Copying buffer to input_buffer_all_transfers_copied')\n",
    "                \n",
    "                input_buffer_all_transfers_copied.append( \n",
    "                    input_buffer[:int(BUFFER_ITEM_CAPACITY * INPUT_BUFFER_LOCATIONS_PER_ITEM+1)].copy() )\n",
    "                if debug:\n",
    "                    print(f'Buffer was copied')\n",
    "            \n",
    "#             # TODO: FIX, THIS IS GOING TO CAUSE PROBLEMS\n",
    "#             if (items_transferred) != TLAST_INTERVAL:\n",
    "#                 print(f'All DMA transfers completed (no parsing), total_items={total_items}. It is assumed that all transfers completed because items_transferred ({items_transferred}) != TLAST_INTERVAL ({TLAST_INTERVAL}).')\n",
    "#                 return None\n",
    "            continue \n",
    "            \n",
    "        if debug:\n",
    "            print(f'Transfer {i} finished, items_transferred={items_transferred}, parsing...')\n",
    "        df_metrics_single = parse_input_buffer(input_buffer, items_transferred, packet_format=packet_format)\n",
    "        df_metrics_single = preprocess_df_metrics(df_metrics_single, dont_decode=dont_decode)\n",
    "        df_metrics = df_metrics.append(df_metrics_single, ignore_index=True)\n",
    "        total_execution_clocks += df_metrics_single['clk_counter_halt_agnostic'].sum()\n",
    "\n",
    "#         if df_metrics_single['instr_names'][-1].lower() == 'wfi':\n",
    "#             break\n",
    "            \n",
    "        execution_time_ms = (total_execution_clocks / CLK_SPEED * 1000)\n",
    "        if execution_time_limit is not None and execution_time_limit < execution_time_ms:\n",
    "            print(f'Execution time limit ({execution_time_limit}ms) was reached, tracing is stopped. (execution time={execution_time_ms}ms)')\n",
    "            break\n",
    "        if debug:\n",
    "            print(f'execution_time_ms = {execution_time_ms}')\n",
    "            \n",
    "    if debug:\n",
    "        print(f'All DMA transfers completed, total_items={total_items}.') \n",
    "    df_metrics = postprocess_df_metrics(df_metrics, columns_order=columns_order)\n",
    "    return df_metrics\n",
    "\n",
    "def run_and_collect(stdin, input_buffer=input_buffer, dont_decode=False, dont_parse=False, copy_collected=False, \n",
    "                    execution_time_limit=None, packet_format=Packet_Format.data_pkt, \n",
    "                    columns_order=DataFrame_Columns_Order.data_pkt_columns, \n",
    "                    debug=False):\n",
    "    ''' dont_decode=True saves time (otherwise instruction assembly string is created from hex instruction value) '''\n",
    "    # set CPU into inactive state (active-low reset is set LOW)\n",
    "    gpio_rst_n_out.write(0)\n",
    "    # activate continous_monitoring_system in case if it's stopped by previously \n",
    "    # encountered \"wait for interrupt\" (WFI) instruction\n",
    "    cms_ctrl.reset_wfi_wait()\n",
    "    # send standard input into a buffer, this way it will be ready\n",
    "    # immediately after CPU starts running the program    \n",
    "    console_io.send(stdin, end_byte=ord('\\n')) # '\\n' is hardcoded here specifically for \"stack-mission.c\" program\n",
    "    \n",
    "    # get transfer and ignore it just in case if internal trace storage is not empty\n",
    "    items_transferred = get_dma_transfer(input_buffer, dma_rec)\n",
    "    \n",
    "    reset_cpu()\n",
    "    df = collect_program_data(input_buffer, dont_decode=dont_decode, dont_wait=False, dont_parse=dont_parse, \n",
    "                              copy_collected=copy_collected, execution_time_limit=execution_time_limit, \n",
    "                              packet_format=packet_format, columns_order=columns_order, debug=debug)\n",
    "    stdout = console_io.read()\n",
    "    return df, stdout\n",
    "    \n",
    "def get_performance_stats(df, clk_speed=50_000_000):\n",
    "    halted_time = df['fifo_full_ticks_counts'][1:].sum() / clk_speed\n",
    "    normal_run_time = df['clk_counter'][1:].sum() / clk_speed - halted_time\n",
    "    performance_decrease = 100.0 - normal_run_time / (normal_run_time + halted_time) * 100\n",
    "    return halted_time, normal_run_time, performance_decrease\n",
    "\n",
    "def print_performance_stats(df):\n",
    "    halted_time, normal_run_time, performance_decrease = get_performance_stats(df)\n",
    "    print(f'normal_run_time = {normal_run_time}s')\n",
    "    print(f'halted_time = {halted_time}s')\n",
    "    print(f'performance_decrease = {performance_decrease}%')\n",
    "\n",
    "    \n",
    "def get_available_disk_space(human_readable=False):\n",
    "    if human_readable:\n",
    "        return os.popen('df -h --output=avail,source | grep root').read().strip().split(' ')[0]\n",
    "    return int(os.popen('df --output=avail,source | grep root | cut -d \" \" -f 1').read().strip())\n",
    "\n",
    "def reorder_df_columns(df, columns_order):\n",
    "    ''' function to reorder specified columns, columns not mentioned\n",
    "    in columns_order will be appended to the end of the dataframe '''\n",
    "    columns = df.columns.tolist()\n",
    "    for column in columns_order:\n",
    "        assert column in columns, f'ERROR: reorder_df_columns \"{column}\" column was not found in columns (columns={columns})'\n",
    "        columns.remove(column)\n",
    "    columns = columns_order + columns\n",
    "    return df[columns]\n",
    "\n",
    "event_names = read_performance_event_names(PERFORMANCE_EVENTS_FNAME)\n",
    "\n",
    "# mem = !cat /proc/meminfo | grep 'MemFree'\n",
    "# print(mem)\n",
    "\n",
    "# set processor into reset state\n",
    "gpio_rst_n_out.write(0)\n",
    "\n",
    "# load the default program\n",
    "#bram_loader.start_load(PROGRAMS_DIR / 'ECG/ecg.bin')\n",
    "# bram_loader.start_load(PROGRAMS_DIR / 'HW/hello_world.bin')\n",
    "# while not bram_loader.finished_loading():\n",
    "#     print(bram_loader.get_load_progress())\n",
    "#     time.sleep(1)\n",
    "\n",
    "print()\n",
    "print('Initialization done')\n",
    "print()\n",
    "print(operational_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcp_server.set_verbose(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration of CMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings of continuous monitoring system like:\n",
    "* monitored range (optional)\n",
    "* monitoring start trigger address (optional)\n",
    "* monitoring end trigger address (optional)\n",
    "* auto-halting cpu when internal trace storage is full (optional)\n",
    "* trace filtering method (basic, watchpoint-based, external-pin triggered)\n",
    "* set watchpoints conditions upon which data is collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Continuous Monitoring System Controller configuration:\n",
      "    trigger_trace_start_address_enabled: False\n",
      "    trigger_trace_end_address_enabled: False\n",
      "    trigger_trace_start_address: 0x1000\n",
      "    trigger_trace_end_address: 0x80000106\n",
      "    monitored_address_range_lower_bound_enabled: False\n",
      "    monitored_address_range_upper_bound_enabled: False\n",
      "    monitored_address_range_lower_bound: 0xfff\n",
      "    monitored_address_range_upper_bound: 0x800000ff\n",
      "    basic_trace_filter_mode: ALL_INSTRUCTIONS\n",
      "    basic_trace_filter_time_interval_ticks: 1000\n",
      "    basic_trace_filter_time_interval_type: 1\n",
      "    external_trace_filter_mode_enabled: False\n",
      "    feature_extractor_halting_cpu_enabled: False\n",
      "    atf_mode: ANOMALY_DETECTION\n",
      "    atf_active: True\n",
      "    tlast_interval: 0\n",
      "    halting_cpu: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def setup_cms(cms_ctrl):\n",
    "    \n",
    "    cms_ctrl.disable_storing_config() \n",
    "    # Triggerring (exact address must match to start/stop trace)\n",
    "    cms_ctrl.set_trigger_trace_start_address(0x1000)\n",
    "    cms_ctrl.set_trigger_trace_end_address(0x80000106)  \n",
    "    cms_ctrl.set_trigger_trace_start_address_enabled(False)\n",
    "    cms_ctrl.set_trigger_trace_end_address_enabled(False)\n",
    "\n",
    "    # Filtering (any address between lower bound and upper bound will be collected)\n",
    "    cms_ctrl.set_monitored_address_range_lower_bound(0x0FFF)     #(0x80000000)\n",
    "    cms_ctrl.set_monitored_address_range_upper_bound(0x800000FF)\n",
    "    cms_ctrl.set_monitored_address_range_lower_bound_enabled(False)\n",
    "    cms_ctrl.set_monitored_address_range_upper_bound_enabled(False)\n",
    "    \n",
    "    # Allow further trace collection if last traced program used \"wfi\"\n",
    "    # (wait for interrupt) instruction which stops the trace.\n",
    "    cms_ctrl.reset_wfi_wait()\n",
    "    cms_ctrl.set_tlast_interval(TLAST_INTERVAL)\n",
    "    \n",
    "    \n",
    "    # CPU HALTING IS DISABLED TO PREVENT HALT ON TOO FREQUENT WATCHPOINT MATCHES\n",
    "    # As described in: https://trello.com/c/6eHG0Siu/17-prevent-cpu-halt-on-too-frequent-watchpoints\n",
    "    # IF THE FIFO IS FULL, NOTIFY THE GUI THROUGH \"add_points\" MESSAGE OR ANY OTHER \n",
    "    # SPECIAL MESSAGE\n",
    "    \n",
    "#      cms_ctrl.enable_halting_cpu()\n",
    "    cms_ctrl.disable_halting_cpu()\n",
    "    \n",
    "    cms_ctrl.reset_atf()\n",
    "    \n",
    "    # 3 MAIN TRACE FILTER OPTIONS:\n",
    "    # - basic\n",
    "    # - advanced (watchpoint-based)\n",
    "    # - external (data is collected when external input to CMS is high)\n",
    "       \n",
    "    # Basic trace filter configuration (affects atf mode too)\n",
    "#     cms_ctrl.set_basic_trace_filter_mode_jump_branch_return()\n",
    "    cms_ctrl.set_basic_trace_filter_mode_all_instructions()\n",
    "    # cms_ctrl.set_basic_trace_filter_mode_time_interval()\n",
    "    # cms_ctrl.set_basic_trace_filter_time_interval_ticks(1)\n",
    "    \n",
    "    #cms_ctrl.set_basic_trace_filter_mode(BASIC_TRACE_FILTER_MODE.DISABLED)\n",
    "    \n",
    "    # Advanced trace filter (ATF) configuration\n",
    "    # DIRECT MATCH ATF WATCHPOINTS (determining when data is collected):\n",
    "    #cms_ctrl.set_atf_match_watchpoint(0, {'pc':0x8000076c})\n",
    "    #cms_ctrl.set_atf_match_watchpoint(0, {'pc':0x80000760}) # ecg_baseline wait_ms\n",
    "    #cms_ctrl.set_atf_match_watchpoint(0, {'pc':0x800008B0}) # ecg_baseline wait_ms_2\n",
    "    cms_ctrl.set_atf_mode(ATF_MODE.ANOMALY_DETECTION) # alternative: ATF_MODE.PATTERN_COLLECTION\n",
    "    cms_ctrl.enable_atf()\n",
    "    #cms_ctrl.disable_atf() \n",
    "    \n",
    "    cms_ctrl.disable_external_trace_filter() \n",
    "    \n",
    "    #cms_ctrl.enable_feature_extractor_halting_cpu()\n",
    "    cms_ctrl.disable_feature_extractor_halting_cpu()\n",
    "    \n",
    "    cms_ctrl.enable_storing_config()\n",
    "    cms_ctrl.store_config()\n",
    "\n",
    "\n",
    "def set_watchpoint_conditions(conditions):\n",
    "    cms_ctrl.reset_atf()\n",
    "    for i, wp in enumerate(conditions): # make sure the number of chosen conditions is not too many (max 8 as of 12/12/2023) \n",
    "        cms_ctrl.set_atf_match_rule(i, values_dict={}, seed=wp['seed'], mask=wp['mask'], bits_to_use=wp['bits_to_use'])\n",
    "\n",
    "def setup_cms_for_testing_feature_extractor():\n",
    "    cms_ctrl.set_basic_trace_filter_mode(BASIC_TRACE_FILTER_MODE.DISABLED)\n",
    "    cms_ctrl.disable_atf()\n",
    "    # \"valid\" output of feature extractor is the external_trace_filter_keep_item signal of CMS\n",
    "    cms_ctrl.enable_external_trace_filter() \n",
    "    cms_ctrl.enable_feature_extractor_halting_cpu()\n",
    "    cms_ctrl.enable_halting_cpu()\n",
    "\n",
    "    \n",
    "# setup_cms(cms_ctrl)\n",
    "\n",
    "# no need to setup_cms anymore because it's loaded/stored\n",
    "cms_ctrl.print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCP server setup\n",
    "Functions that start with \"rpc_\" can be called from the Esp32 (with a display)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCP server can be accessed at: 192.168.0.110:9093\n",
      "Server -> Client:\n",
      "{'status_update': {'atf_watchpoints': {'0': {'active': True,\n",
      "                                             'attributes': {'pc': '800002f0'},\n",
      "                                             'attributes_notes': {'pc': 'wait_ms'}},\n",
      "                                       '1': {'active': True,\n",
      "                                             'attributes': {'pc': '80000248'},\n",
      "                                             'attributes_notes': {'pc': 'is_barcode_valid'}},\n",
      "                                       '2': {'active': False,\n",
      "                                             'attributes': {'pc': '80000314'},\n",
      "                                             'attributes_notes': {'pc': 'wait_ms+0x24'}}},\n",
      "                   'cms_ctrl_config': {'atf_active': True,\n",
      "                                       'atf_mode': 1,\n",
      "                                       'basic_trace_filter_mode': 1,\n",
      "                                       'basic_trace_filter_time_interval_ticks': 1000,\n",
      "                                       'basic_trace_filter_time_interval_type': 1,\n",
      "                                       'external_trace_filter_mode_enabled': False,\n",
      "                                       'feature_extractor_halting_cpu_enabled': False,\n",
      "                                       'halting_cpu': False,\n",
      "                                       'monitored_address_range_lower_bound': 4095,\n",
      "                                       'monitored_address_range_lower_bound_enabled': False,\n",
      "                                       'monitored_address_range_upper_bound': 2147483903,\n",
      "                                       'monitored_address_range_upper_bound_enabled': False,\n",
      "                                       'tlast_interval': 0,\n",
      "                                       'trigger_trace_end_address': 2147483910,\n",
      "                                       'trigger_trace_end_address_enabled': False,\n",
      "                                       'trigger_trace_start_address': 4096,\n",
      "                                       'trigger_trace_start_address_enabled': False},\n",
      "                   'dataset_size': 0,\n",
      "                   'features_keys': ['pc',\n",
      "                                     'clk_counter',\n",
      "                                     'A0',\n",
      "                                     'A1',\n",
      "                                     'A2',\n",
      "                                     'Core__BRANCH',\n",
      "                                     'Core__JAL',\n",
      "                                     'Core__LOAD',\n",
      "                                     'Core__STORE',\n",
      "                                     'L1I__LD',\n",
      "                                     'L1D__LD',\n",
      "                                     'TGC__WRITE',\n",
      "                                     'TGC__READ'],\n",
      "                   'is_halted': False,\n",
      "                   'is_running': False,\n",
      "                   'loaded_program': 'None',\n",
      "                   'mode': 0,\n",
      "                   'model_has_unsaved_changes': False,\n",
      "                   'model_max_interval': 0.0,\n",
      "                   'model_name_current': 'None',\n",
      "                   'operational_config': {'items_collected_processing_limit': 500,\n",
      "                                          'lack_of_matches_threshold_multiplier_of_max_interval': 1.5,\n",
      "                                          'periodic_send_interval_seconds': 0.8,\n",
      "                                          'raw_data_send_enable': True},\n",
      "                   'program_load_progress': 0,\n",
      "                   'pynq_restarted': True,\n",
      "                   'similarity_threshold': 1.0}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Initially it will be 1, during the calibration phase it will\n",
    "# # be automatically adjusted to lower value (while performing\n",
    "# # the same actions as during training).\n",
    "# similarity_threshold = 1.0\n",
    "\n",
    "# If items collected from fifo is above ITEMS_COLLECTED_PROCESSING_LIMIT, items will not be processed\n",
    "# and data loss will be indicated to GUI controller\n",
    "#\n",
    "# If it happens during training, it means that training results are invalid and \n",
    "# training should be done with less frequently matching watchpoints.\n",
    "# \n",
    "# If it happens during testing, it should be treated as anomaly.\n",
    "# (anomalies in the number of collected items also could be detected,\n",
    "#  but that's another story)\n",
    "# (limit must be not larger than internal FIFO size)\n",
    "# ITEMS_COLLECTED_PROCESSING_LIMIT = 1000\n",
    "# EDIT: it got replaced with operational_config.get_items_collected_processing_limit()\n",
    "\n",
    "# setup database with allowed barcodes\n",
    "db_barcodes = SQL_Barcodes_DB()\n",
    "db_barcodes.reset_table()\n",
    "db_barcodes.init_with_default_barcodes()\n",
    "\n",
    "# lock to protect is_running variable (set in rpc and read in \"operate\" thread)\n",
    "# but also other variables in future if needed\n",
    "server_data_lock = Lock()\n",
    "\n",
    "class MODE:\n",
    "    ''' Operational mode, controlled by TCP client.\n",
    "    This isn't any internal hardware mode, it is just for this PYNQ script, \n",
    "    and to allow the TCP client to control what should happen. '''\n",
    "    # these can be used with bitwise operators (need to be careful if new modes are added)\n",
    "    IDLE = 0\n",
    "    TRAINING = 0b1\n",
    "    TESTING = 0b10\n",
    "    TRAINING_AND_TESTING = 0b11\n",
    "    \n",
    "    # This mode should be done just after training.\n",
    "    # While in this mode, we should repeat all the \n",
    "    # same actions that were done during training\n",
    "    # but this time the lowest encountered similarity\n",
    "    # will become detection threshold for the testing\n",
    "    # mode.\n",
    "    DETECTION_THRESHOLD_CALIBRATION = 0b100 \n",
    "\n",
    "def remove_too_varying_performance_events(events):\n",
    "    events_copy = list(events)\n",
    "    to_remove = ['Core__1_BUSY_NO_CONSUME']\n",
    "    for ev in to_remove:\n",
    "        index = event_names.index(ev)\n",
    "        del events_copy[index]\n",
    "    return events_copy\n",
    "\n",
    "# used_events = remove_too_varying_performance_events(event_names)\n",
    "used_events = event_names\n",
    "\n",
    "USED_PERFORMANCE_EVENTS_COUNT = len(used_events)\n",
    "    \n",
    "# declaration of some variables that are controlled by the client.\n",
    "mode = MODE.IDLE    \n",
    "\n",
    "anomaly_detection = Anomaly_Detection() # model for anomaly detection\n",
    "# Using line below splits datasets into many subdatasets\n",
    "# grouped together by unique program counter values.\n",
    "anomaly_detection.set_special_columns_indices([0])\n",
    "    \n",
    "is_arbitrary_halt_active = False\n",
    "loaded_program = 'None'\n",
    "is_running = False\n",
    "\n",
    "# This multiplier is used for notifying GUI about lack of watchpoint matches.\n",
    "# If there is no data collected within 1.5 * max_recorded_interval_during_training\n",
    "# then lack of matches anomaly is sent.\n",
    "# TODO: must be adjustable and persistent\n",
    "# lack_of_matches_threshold_multiplier_of_max_interval = 1.5\n",
    "# EDIT: it got replaced by operational_config.get_lack_of_matches_threshold_multiplier_of_max_interval()\n",
    "\n",
    "from advanced_trace_filter import ATF_Watchpoints\n",
    "atf_watchpoints = ATF_Watchpoints(cms_ctrl)\n",
    "atf_watchpoints.load_watchpoints()\n",
    "atf_watchpoints.push_all_watchpoints_to_cms()\n",
    "\n",
    "def list_subfolders_with_paths(path):\n",
    "    ''' From: https://stackoverflow.com/a/59938961/4620679 '''\n",
    "    return [f.path for f in os.scandir(path) if f.is_dir()]\n",
    "\n",
    "\n",
    "def generate_status_update_dict():\n",
    "    global anomaly_detection, mode, is_arbitrary_halt_active, loaded_program, is_running, atf_watchpoints\n",
    "    global pynq_restarted, bram_loader, atf_watchpoints, cms_ctrl\n",
    "    # function created to create consistent message for \"status_update\"\n",
    "    # and return of \"rpc_update_status\", so both can be parsed\n",
    "    # using the same routine\n",
    "    return {\n",
    "        'pynq_restarted' : pynq_restarted,\n",
    "        'dataset_size' : anomaly_detection.get_dataset_size(),\n",
    "        'mode' : mode,\n",
    "        'is_halted' : is_arbitrary_halt_active,\n",
    "        'loaded_program' : loaded_program,\n",
    "        'is_running': is_running,\n",
    "        'program_load_progress' : bram_loader.get_load_progress(),\n",
    "        'similarity_threshold' : anomaly_detection.get_similarity_threshold(),\n",
    "        'features_keys' : Packet_Format.get_anomaly_detection_features_names(),\n",
    "        'atf_watchpoints' : atf_watchpoints.get_watchpoints_as_strings(),\n",
    "        'cms_ctrl_config' : cms_ctrl.get_config(),\n",
    "        \n",
    "        # model related\n",
    "        'model_has_unsaved_changes' : anomaly_detection.has_unsaved_changes(),\n",
    "        'model_name_current' : anomaly_detection.get_current_model_name(),\n",
    "        'model_max_interval' : float(anomaly_detection.get_max_interval()),\n",
    "        \n",
    "        # operational config\n",
    "        'operational_config' : operational_config.get_config()\n",
    "    }\n",
    "\n",
    "#############################################################################\n",
    "# API calls for the TCP server (that TCP clients may call whenever they want)\n",
    "def rpc_list_programs():\n",
    "    ''' TCP server API.'''\n",
    "    # key=main program name (dir name in programs) value=list of programs (e.g. ecg_baseline.bin, ecg_ino_leak.bin)\n",
    "    programs = {}\n",
    "    for path in list_subfolders_with_paths(str(PROGRAMS_DIR)):\n",
    "        p_name = os.path.basename(path)\n",
    "        programs[p_name] = sorted([f_name.split('.')[0] for f_name in os.listdir(path) if f_name.endswith(\".bin\")])\n",
    "    return programs\n",
    "    #response = {'programs':programs}\n",
    "    #return json.dumps(response)\n",
    "\n",
    "def rpc_list_objdumps():\n",
    "    objdumps = {}\n",
    "    for path in list_subfolders_with_paths(str(PROGRAMS_DIR)):\n",
    "        p_name = os.path.basename(path)\n",
    "        objdump_path = Path(path) / 'objdump'\n",
    "        objdumps[p_name] = sorted([f_name.split('.')[0] for f_name in os.listdir(objdump_path) if f_name.endswith(\".dump\")])\n",
    "    return objdumps\n",
    "\n",
    "def rpc_get_objdump_data(category, objdump_fname):\n",
    "    # {'_start': {'80000000': {'name': 'entry', 'type': 'entry'},\n",
    "    #             '80000004': {'branch_destination': '<park>',\n",
    "    #                          'name': 'BNEZ',\n",
    "    #                          'type': 'branch'},\n",
    "    #             '80000010': {'branch_destination': '<main>',\n",
    "    #                          'name': 'J',\n",
    "    #                          'type': 'branch'}},\n",
    "    # 'main': {'80000038': {'name': 'entry', 'type': 'entry'},\n",
    "    #           '80000088': {'branch_destination': '<main+0x6c>', 'name': 'BEQZ', 'type': 'branch'},\n",
    "    #           '80000094': {'name': 'uart_gpio_puts', 'type': 'function'},    \n",
    "    if not objdump_fname.endswith('.dump'):\n",
    "        objdump_fname += '.dump'\n",
    "        \n",
    "    full_fname = PROGRAMS_DIR / Path(category) / f'objdump/{objdump_fname}'\n",
    "    try:\n",
    "        return parse_objdump(full_fname)\n",
    "    except Exception as e:\n",
    "        error_msg = f'ERROR: failed parsing \"{full_fname}\" file: ' + str(e)\n",
    "        print(error_msg)\n",
    "        return error_msg\n",
    "\n",
    "def rpc_load_program(name):\n",
    "    ''' TCP server API. '''\n",
    "    global is_arbitrary_halt_active, loaded_program\n",
    "    if not name.endswith('.bin'):\n",
    "        name += '.bin'\n",
    "    for dirpath, d_names, f_names in os.walk(str(PROGRAMS_DIR)):\n",
    "        for f_name in f_names:\n",
    "            if f_name != name:\n",
    "                continue\n",
    "            full_path = os.path.join(dirpath, name)\n",
    "            gpio_rst_n_out.write(0)\n",
    "            bram_loader.start_load(full_path)\n",
    "            while not bram_loader.finished_loading():\n",
    "                send_file_load_progress(bram_loader.get_load_progress())\n",
    "                time.sleep(0.5)\n",
    "            send_file_load_progress(bram_loader.get_load_progress())\n",
    "            \n",
    "            if is_arbitrary_halt_active:\n",
    "                cms_ctrl.deactivate_arbitrary_halt()\n",
    "                is_arbitrary_halt_active = False\n",
    "            loaded_program = name.split('.')[0]\n",
    "            return f\"OK: loaded {name} program.\"\n",
    "            #return json.dumps({'status_update': f'OK: ran {name} program'})\n",
    "    return f\"ERROR: didn't find {name} program\"\n",
    "\n",
    "def rpc_run():\n",
    "    ''' TCP server API. '''\n",
    "    global is_arbitrary_halt_active, is_running, server_data_lock\n",
    "    if not is_arbitrary_halt_active:\n",
    "        reset_cpu()\n",
    "    else:\n",
    "        cms_ctrl.deactivate_arbitrary_halt()\n",
    "        is_arbitrary_halt_active = False\n",
    "    with server_data_lock:\n",
    "        is_running = True\n",
    "    return \"OK\"\n",
    "\n",
    "def rpc_halt():\n",
    "    ''' TCP server API. '''\n",
    "    global is_arbitrary_halt_active, is_running, server_data_lock\n",
    "    if is_arbitrary_halt_active:\n",
    "        return 'Program was halted anyway'\n",
    "    is_arbitrary_halt_active = True\n",
    "    cms_ctrl.activate_arbitrary_halt()\n",
    "    with server_data_lock:\n",
    "        is_running = False\n",
    "    return 'CPU halted'\n",
    "    \n",
    "def rpc_enable_training():\n",
    "    global mode\n",
    "    mode |= MODE.TRAINING\n",
    "    return mode\n",
    "    \n",
    "def rpc_disable_training():\n",
    "    global mode\n",
    "    mode &= ~MODE.TRAINING\n",
    "    return mode\n",
    "\n",
    "def rpc_enable_testing():\n",
    "    global mode\n",
    "    mode |= MODE.TESTING\n",
    "    return mode\n",
    "\n",
    "def rpc_disable_testing():\n",
    "    global mode\n",
    "    mode &= ~MODE.TESTING\n",
    "    return mode\n",
    "\n",
    "def rpc_enable_detection_threshold_calibration():\n",
    "    global mode\n",
    "    mode |= MODE.DETECTION_THRESHOLD_CALIBRATION\n",
    "    return mode\n",
    "    \n",
    "def rpc_disable_detection_threshold_calibration():\n",
    "    global mode\n",
    "    mode &= ~MODE.DETECTION_THRESHOLD_CALIBRATION\n",
    "    return mode\n",
    "\n",
    "def rpc_reset_dataset():\n",
    "    global anomaly_detection\n",
    "    anomaly_detection.reset_dataset()\n",
    "    return 'Dataset resetted'\n",
    "\n",
    "def rpc_update_status():\n",
    "    return generate_status_update_dict()\n",
    "#             {'dataset_size' : anomaly_detection.get_dataset_size(),\n",
    "#             'mode' : mode,\n",
    "#             'is_halted' : is_arbitrary_halt_active,\n",
    "#             'loaded_program' : loaded_program,\n",
    "#             'is_running': is_running,\n",
    "#             'atf_watchpoints' : atf_watchpoints.get_watchpoints_as_strings()}\n",
    "\n",
    "def rpc_set_atf_watchpoint(index, is_active, json_str_attributes_dict, json_str_attributes_notes_dict={}):\n",
    "    global atf_watchpoints\n",
    "    if type(is_active) == str:\n",
    "        is_active = (is_active.lower() == 'true' or is_active == '1')\n",
    "    index = int(index)\n",
    "        \n",
    "    try:\n",
    "        attributes_dict = json.loads(json_str_attributes_dict)\n",
    "    except Exception as e:\n",
    "        error_msg = 'ERROR: rpc_set_atf_watchpoint: ' + str(e)\n",
    "        print(error_msg)\n",
    "        return error_msg\n",
    "    try:\n",
    "        attributes_notes_dict = json.loads(json_str_attributes_notes_dict)\n",
    "    except Exception as e:\n",
    "        error_msg = 'ERROR: rpc_set_atf_watchpoint (setting attributes_notes_dict): ' + str(e)\n",
    "        print(error_msg)\n",
    "        attributes_notes_dict = {}\n",
    "    atf_watchpoints.set_watchpoint(index, attributes_dict, is_active, attributes_notes_dict=attributes_notes_dict)\n",
    "    return f\"OK_{index}\"\n",
    "\n",
    "def rpc_atf_watchpoint_set_active(index, state):\n",
    "    global atf_watchpoints\n",
    "    print(f'rpc_atf_watchpoint_set_active index={index} state={state}')\n",
    "    \n",
    "    if type(is_active) == str:\n",
    "        is_active = (is_active.lower() == 'true' or is_active == '1')\n",
    "    index = int(index)\n",
    "    success = atf_watchpoints.set_watchpoint_active(index, state)\n",
    "    return \"OK\" if success else f\"WARNING: Watchpoint with index={index} wasn't there\"\n",
    "\n",
    "def rpc_remove_atf_watchpoint(index):\n",
    "    global atf_watchpoints\n",
    "    index = int(index)\n",
    "    success = atf_watchpoints.remove_watchpoint(index)\n",
    "    return \"OK\" if success else f\"WARNING: Watchpoint with index={index} wasn't there\"\n",
    "\n",
    "def rpc_set_similarity_threshold(threshold):\n",
    "    global anomaly_detection\n",
    "    anomaly_detection.set_similarity_threshold(threshold)\n",
    "    return anomaly_detection.get_similarity_threshold()\n",
    "\n",
    "def rpc_list_available_models():\n",
    "    return anomaly_detection.list_datasets()\n",
    "\n",
    "def rpc_save_detection_model(name):\n",
    "    anomaly_detection.store_dataset(name)\n",
    "    return \"OK\"\n",
    "    \n",
    "def rpc_load_detection_model(name):\n",
    "    anomaly_detection.load_dataset(name)\n",
    "    return \"OK\"\n",
    "\n",
    "def rpc_send_stdin(stdin_str):\n",
    "    console_io.send(stdin_str)\n",
    "    return \"OK\"\n",
    "\n",
    "def rpc_read_stdout():\n",
    "    # if this is going to be implemented for some reason (e.g. viewing stdout in GUI)\n",
    "    # then stdin_stdout_communication() function will need to store received stdout\n",
    "    # in some global variable or some object\n",
    "    return console_io.read()\n",
    "\n",
    "def rpc_readlines_stdout():\n",
    "    return console_io.read().split('\\n')\n",
    "\n",
    "def rpc_set_cms_ctrl_attributes(json_str_attributes_dict):\n",
    "    try:\n",
    "        attributes_dict = json.loads(json_str_attributes_dict)\n",
    "    except Exception as e:\n",
    "        error_msg = 'ERROR: rpc_set_cms_ctrl_attributes: ' + str(e)\n",
    "        print(error_msg)\n",
    "        return error_msg\n",
    "    try:\n",
    "        errors = cms_ctrl.update_attributes(attributes_dict)\n",
    "    except Exception as e:\n",
    "        error_msg = 'ERROR: rpc_set_cms_ctrl_attributes: ' + str(e)\n",
    "        print(error_msg)\n",
    "        return error_msg\n",
    "    return 'OK' if not errors else errors\n",
    "\n",
    "def rpc_set_model_max_interval(interval):\n",
    "    global anomaly_detection\n",
    "    anomaly_detection.set_max_interval(interval)\n",
    "    return 'OK'\n",
    "\n",
    "def rpc_set_operational_config(json_str_attributes_dict):\n",
    "    try:\n",
    "        attributes_dict = json.loads(json_str_attributes_dict)\n",
    "    except Exception as e:\n",
    "        error_msg = 'ERROR: rpc_set_operational_config: ' + str(e)\n",
    "        print(error_msg)\n",
    "        return error_msg\n",
    "    try:\n",
    "        errors = operational_config.update_attributes(attributes_dict)\n",
    "    except Exception as e:\n",
    "        error_msg = 'ERROR: rpc_set_operational_config: ' + str(e)\n",
    "        print(error_msg)\n",
    "        return error_msg\n",
    "    return 'OK' if not errors else errors\n",
    "\n",
    "# def rpc_set_periodic_send_interval_seconds(val):\n",
    "#     global operational_mode\n",
    "#     operational_mode.set_periodic_send_interval_seconds()\n",
    "#     return 'OK'\n",
    "\n",
    "# def rpc_set_items_collected_processing_limit(val):\n",
    "#     global operational_mode\n",
    "#     operational_mode.set_items_collected_processing_limit(val)\n",
    "#     return 'OK'\n",
    "\n",
    "# def rpc_set_lack_of_matches_threshold_multiplier_of_max_interval(val):\n",
    "#     global operational_mode\n",
    "#     operational_mode.set_lack_of_matches_threshold_multiplier_of_max_interval(val)\n",
    "#     return 'OK'\n",
    "    \n",
    "#############################################################################\n",
    "# Functions that the PYNQ board can use to notify all clients about \n",
    "\n",
    "def send_sensors_data(df_sensors, sensors_to_send):\n",
    "    msg_to_server = ''\n",
    "    for i in range(df_sensors.shape[0]):\n",
    "        for col in sensors_to_send: #df_sensors.columns:\n",
    "            val = float(df_sensors[col].iloc[i]) / 60000.0\n",
    "            msg_to_server += f'add_point:{col},{val}\\n'\n",
    "    #print(msg_to_server)\n",
    "    tcp_server.send_to_all(msg_to_server) \n",
    "\n",
    "def send_file_load_progress(percent):\n",
    "    tcp_server.send_to_all(json.dumps({\n",
    "        'status_update' : {\n",
    "            'program_load_progress' : percent\n",
    "        }\n",
    "    }))\n",
    "\n",
    "def send_similarity_threshold(threshold):\n",
    "    tcp_server.send_to_all(json.dumps({\n",
    "        'status_update' : {\n",
    "            'similarity_threshold' : float(threshold)\n",
    "        }\n",
    "    }))\n",
    "\n",
    "def send_model_unsaved_changes(state):\n",
    "    tcp_server.send_to_all(json.dumps({\n",
    "        'status_update' : {\n",
    "            'model_has_unsaved_changes' : state\n",
    "        }\n",
    "    }))\n",
    "\n",
    "def send_model_current_name(name):\n",
    "    tcp_server.send_to_all(json.dumps({\n",
    "        'status_update' : {\n",
    "            'model_name_current' : name\n",
    "        }\n",
    "    }))\n",
    "\n",
    "def send_model_max_interval(interval):\n",
    "    tcp_server.send_to_all(json.dumps({\n",
    "        'status_update' : {\n",
    "            'model_max_interval' : float(interval)\n",
    "        }\n",
    "    }))\n",
    "\n",
    "def send_stdout(msg):\n",
    "    if not msg: return\n",
    "    tcp_server.send_to_all(json.dumps({\n",
    "        'stdout' : msg\n",
    "    }))\n",
    "\n",
    "def send_stdin(msg):\n",
    "    if not msg: return\n",
    "    tcp_server.send_to_all(json.dumps({\n",
    "        'stdin' : msg\n",
    "    }))\n",
    "\n",
    "# PERIODIC_SEND_INTERVAL = 0.8 # in seconds \n",
    "# EDIT: replaced by operational_config.get_periodic_send_interval_seconds()\n",
    "\n",
    "def send_periodic_update(similarities, items_since_last_send, clk_time_since_last_send, halted_time_since_last_send, \n",
    "                         mode, send_dataset_size=False, processing_limit_exceeded=False, lack_of_matches=False):\n",
    "    global anomaly_detection, operational_config\n",
    "    \n",
    "    if mode & MODE.TESTING:\n",
    "        number_of_anomalies = sum(1 for s in similarities if s < anomaly_detection.get_similarity_threshold())\n",
    "    else:\n",
    "        number_of_anomalies = 0    \n",
    "        \n",
    "    \n",
    "    if processing_limit_exceeded or lack_of_matches:\n",
    "        # processing was not done in this case and all collected items were ignored\n",
    "        avg_sim_bot_1 = 0.0\n",
    "        avg_sim = 0.0\n",
    "        performance_rate = 1.0 \n",
    "    else:\n",
    "        avg_sim_bot_1 = 1 if not similarities else np.mean( sorted(similarities)[:math.ceil(len(similarities)/100)] )\n",
    "        avg_sim = 1 if not similarities else np.mean(similarities)\n",
    "    #     total_exec_time = clk_time_since_last_send + halted_time_since_last_send\n",
    "    #     print('total_exec_time =', total_exec_time)\n",
    "    #     print('clk_time_since_last_send =', clk_time_since_last_send)\n",
    "    #     print('halted_time_since_last_send =', halted_time_since_last_send)\n",
    "        performance_rate = (1 - halted_time_since_last_send / (clk_time_since_last_send or 1)) # \"or 1\" prevents division by 0\n",
    "        \n",
    "    dict_ = {\n",
    "        'add_points' : {\n",
    "            'Perf' : [performance_rate],\n",
    "            'Avg sim' : [avg_sim],\n",
    "            'Avg sim bot-1%' : [avg_sim_bot_1],\n",
    "            'Items collected' : [items_since_last_send],\n",
    "            'Anomalies' : [number_of_anomalies],\n",
    "            'similarity_threshold' : [anomaly_detection.get_similarity_threshold()],\n",
    "            'data_loss' : [1.0 if processing_limit_exceeded else 0.0],\n",
    "            'data_loss_2' : [1.0 if processing_limit_exceeded else 0.0], # for displaying on 2nd graph\n",
    "            'Items collected processing limit' : [float(operational_config.get_items_collected_processing_limit())],\n",
    "            'lack_of_matches' : [1.0 if lack_of_matches else 0.0],\n",
    "            'lack_of_matches_2' : [1.0 if lack_of_matches else 0.0]  # for displaying on 2nd graph\n",
    "        }\n",
    "    }\n",
    "    if send_dataset_size:\n",
    "        size = anomaly_detection.get_dataset_size()\n",
    "        dict_['status_update'] : {'dataset_size' : size}\n",
    "        dict_['add_points']['dataset_size'] = [size]\n",
    "    \n",
    "    tcp_server.send_to_all(\n",
    "        json.dumps(dict_)\n",
    "    )\n",
    "\n",
    "def send_new_anomaly(metrics_dict, similarity, features_vector, most_similar_vector, \n",
    "                     data_loss=False, lack_of_matches=False):  \n",
    "    if not data_loss and not lack_of_matches:\n",
    "        halt_agnostic_clk_counter = metrics_dict['clk_counter'] - metrics_dict['fifo_full_ticks_count']\n",
    "        if most_similar_vector is None:\n",
    "            # most_similar_vector can be None if the PC of features_vector isn't found \n",
    "            # at all in the dataset (because dataset is split into subdatasets grouped\n",
    "            # by PC), in that case all \"-1s\" are sent \n",
    "            most_similar_vector = np.full_like(features_vector, -1.0)\n",
    "\n",
    "        pc = f\"0x{metrics_dict['pc']:X}\"\n",
    "    else:\n",
    "        # if too many watchpoints matched (and processing wasn't done)\n",
    "        features_count = len(Packet_Format.get_anomaly_detection_features_names())\n",
    "        pc = \"-\"\n",
    "        halt_agnostic_clk_counter = '-'\n",
    "        similarity = 0\n",
    "        features_vector = [-1] * features_count\n",
    "        most_similar_vector = features_vector\n",
    "        \n",
    "        \n",
    "    tcp_server.send_to_all(json.dumps({\n",
    "        'new_anomaly' : {\n",
    "            'pc' : pc,                                  # string\n",
    "            'time' : datetime.datetime.now().strftime(\"%H:%M:%S\"),               # string\n",
    "            'total_clk_counter' : str(halt_agnostic_clk_counter),                # string\n",
    "            'similarity' : similarity,                                           # float between 0 and 1\n",
    "            'features_vector' : list(float(v) for v in features_vector),         # list of floats\n",
    "            'most_similar_vector' : list(float(v) for v in most_similar_vector),  # list of floats\n",
    "            'data_loss' : data_loss,\n",
    "            'lack_of_matches' : lack_of_matches\n",
    "        }\n",
    "    }))\n",
    "    \n",
    "def send_raw_data(raw_data_dict):\n",
    "    try:\n",
    "        tcp_server.send_to_all(json.dumps({\n",
    "            'raw_data_dict' : raw_data_dict\n",
    "        }))\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR send_raw_data: {e}\")\n",
    "        print(raw_data_dict)\n",
    "\n",
    "# all functions from this file that start with \"rpc_\"\n",
    "all_rpcs = [func for name,func in inspect.getmembers(sys.modules[__name__]) if (inspect.isfunction(func) and name.startswith('rpc_'))]\n",
    "tcp_server.register_rpcs(all_rpcs)\n",
    "\n",
    "tcp_server.start()\n",
    "print(f'TCP server can be accessed at: {get_my_ip()}:{TCP_SERVER_PORT}')\n",
    "\n",
    "tcp_server.send_to_all(json.dumps({'status_update': generate_status_update_dict()}))\n",
    "pynq_restarted = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tcp_server.set_verbose(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main operation\n",
    "Code part below should run a loop that will collect program metrics and depending on the state set by the TCP client:\n",
    "- update model with training data\n",
    "- calculate similarity to trained model and update client about it\n",
    "- update client with metrics collection status\n",
    "\n",
    "\n",
    "Similarity data may sent to client may be:\n",
    "- the number of anomalous items collected (e.g. with similarity < 0.9)\n",
    "- average similarity since last update (not sure, this may just be a distraction from the number of anomalies which is the most important)\n",
    "\n",
    "\n",
    "Metrics collection status may include:\n",
    "- the number of collected items since last update\n",
    "- performance rate (total_execution_time - halted_time / total_execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "chunks_per_item = math.ceil(AXI_DATA_WIDTH/64)\n",
    "# CLK_LOCATION = PERFORMANCE_EVENTS_COUNT * PERFORMANCE_COUNTER_WIDTH + PERFORMANCE_COUNTERS_OVERFLOW_MAP_WIDTH + PC_WIDTH\n",
    "# HALTED_CLK_LOCATION = CLK_LOCATION + CLK_COUNTER_WIDTH + INSTR_WIDTH\n",
    "\n",
    "# variables below allow to interact with the thread running \"operate\" function\n",
    "end_operate_thread = False\n",
    "print_stats = False\n",
    "\n",
    "#mode = MODE.TRAINING\n",
    "#mode = MODE.TESTING\n",
    "#mode = MODE.TRAINING_AND_TESTING\n",
    "\n",
    "similarities = []\n",
    "\n",
    "console_io.read() # ignore all previous stdout\n",
    "\n",
    "console_io.set_on_send_callback(lambda msg: send_stdin(msg))\n",
    "console_io.set_on_receive_callback(lambda msg: send_stdout(msg))\n",
    "\n",
    "anomaly_detection.set_on_has_unsaved_changes_callback(lambda state: send_model_unsaved_changes(state))\n",
    "anomaly_detection.set_on_model_current_name_callback(lambda name: send_model_current_name(name))\n",
    "anomaly_detection.set_on_max_interval_change_callback(lambda interval: send_model_max_interval(interval))\n",
    "\n",
    "not_processed_stdin = \"\"\n",
    "def stdin_stdout_communication():\n",
    "    global not_processed_stdin\n",
    "    try:\n",
    "        stdout_whole_str = console_io.read()\n",
    "        if not stdout_whole_str:\n",
    "            return\n",
    "#         send_stdout(stdout_whole_str)\n",
    "        lines = stdout_whole_str.split('\\n')\n",
    "        if not lines:\n",
    "            return\n",
    "        # don't process stdin that does not end with '\\n'\n",
    "        if not_processed_stdin:\n",
    "            lines[0] = not_processed_stdin + lines[0]\n",
    "            not_processed_stdin = \"\"\n",
    "        # if last line is not empty, it means that\n",
    "        # '\\n' wasn't at the end of the received text\n",
    "        # so the last string should be saved for further processing\n",
    "        if lines[-1] != \"\":\n",
    "            not_processed_stdin = str(lines[-1])\n",
    "        # either way the last line must be removed because either it's:\n",
    "        # - empty (due to text ending with '\\n')\n",
    "        # - not complete message that should not be processed (due to lack of '\\n' at the end)\n",
    "        del lines[-1]\n",
    "       \n",
    "        for line in lines:\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\"bc:\"):\n",
    "                bc = line.split(\"bc:\")[1]\n",
    "                is_in_db = 1 if db_barcodes.is_barcode_in_db(bc) else 0\n",
    "                console_io.send(f'bc:{is_in_db}', end_byte=ord('\\n'))\n",
    "            print(line)\n",
    "    except UnicodeDecodeError:\n",
    "        print('WARNING: UnicodeDecodeError in stdin_stdout_communication')\n",
    "\n",
    "def operate():\n",
    "    global end_operate_thread, print_stats\n",
    "    global anomaly_detection\n",
    "    global server_data_lock\n",
    "    global is_running\n",
    "    global operational_config\n",
    "    \n",
    "    # items received through DMA\n",
    "    items_transferred = 0\n",
    "    total_clk_time = 0\n",
    "    total_halted_time = 0\n",
    "    \n",
    "    # metrics for client (display)\n",
    "    items_since_last_send = 0\n",
    "    clk_time_since_last_send = 0\n",
    "    halted_time_since_last_send = 0\n",
    "    last_send_time = time.time()\n",
    "    processing_limit_exceeded = False\n",
    "    # to prevent spamming send_new_anomaly\n",
    "    processing_limit_exceeded_reseted = True\n",
    "    \n",
    "    # to recognize lack of watchpoint matches\n",
    "    last_watchpoint_match_time = 0\n",
    "    lack_of_matches = False\n",
    "    # to prevent spamming send_new_anomaly\n",
    "    lack_of_matches_reset = True\n",
    "    \n",
    "    while True:        \n",
    "        if end_operate_thread:\n",
    "            print('Exiting thread')\n",
    "            return\n",
    "        stdin_stdout_communication()\n",
    "        \n",
    "        if time.time() - last_send_time > operational_config.get_periodic_send_interval_seconds() and mode != MODE.IDLE:\n",
    "            # send dataset_size update only if training or calibration is enabled\n",
    "            send_dataset_size = (mode & MODE.TRAINING) or (mode & MODE.DETECTION_THRESHOLD_CALIBRATION)\n",
    "            send_periodic_update(similarities, items_since_last_send, clk_time_since_last_send, \n",
    "                                 halted_time_since_last_send, mode, send_dataset_size=send_dataset_size,\n",
    "                                 processing_limit_exceeded = processing_limit_exceeded,\n",
    "                                 lack_of_matches = lack_of_matches)\n",
    "            similarities.clear()\n",
    "            items_since_last_send = 0\n",
    "            clk_time_since_last_send = 0\n",
    "            halted_time_since_last_send = 0\n",
    "            last_send_time = time.time()\n",
    "            \n",
    "            processing_limit_exceeded = False\n",
    "\n",
    "        with server_data_lock:\n",
    "            is_running_copy = bool(is_running)\n",
    "        if not is_running_copy or mode == MODE.IDLE:\n",
    "            last_watchpoint_match_time = time.time()\n",
    "            \n",
    "#         # in case of training, setting it to 0 is ok\n",
    "#         # in case of testing, setting it to 0 makes lack_of_matches\n",
    "#         # not being set until at least 1 watchpoint is hit\n",
    "#         if mode & MODE.TESTING:\n",
    "#             last_watchpoint_match_time = time.time()\n",
    "                \n",
    "        items_transferred = get_dma_transfer(input_buffer, dma_rec)\n",
    "        #events, events_overflows, pcs, clk_counters, instrs, instr_names, instr_strings, fifo_full_ticks_counts, all_gp_regs = parse_input_buffer(input_buffer, items_transferred, dont_decode=True)\n",
    "\n",
    "        if not items_transferred:\n",
    "            time.sleep(0.001)\n",
    "            \n",
    "            #if delta > anomaly_detection.get_lack_of_matches_threshold()\n",
    "            if last_watchpoint_match_time and mode & MODE.TESTING:\n",
    "                delta = time.time() - last_watchpoint_match_time\n",
    "                lack_of_matches_threshold = anomaly_detection.get_max_interval() * operational_config.get_lack_of_matches_threshold_multiplier_of_max_interval()\n",
    "                # if threshold was previously set and now exceeded\n",
    "                if lack_of_matches_threshold and delta > lack_of_matches_threshold:\n",
    "                    # if notification wasn't sent yet\n",
    "                    lack_of_matches = True\n",
    "                    if lack_of_matches_reset:\n",
    "                        send_new_anomaly(None, None, None, None, lack_of_matches=lack_of_matches)\n",
    "                        lack_of_matches_reset = False\n",
    "                        \n",
    "#             # don't count intervals between watchpoints when training/calibration is not enabled\n",
    "#             if not mode & MODE.TRAINING and not mode & MODE.DETECTION_THRESHOLD_CALIBRATION:\n",
    "#                 last_watchpoint_match_time = 0\n",
    "            continue\n",
    "        lack_of_matches = False\n",
    "        lack_of_matches_reset = True\n",
    "        \n",
    "        if print_stats:\n",
    "            print(items_transferred, end=', ')\n",
    "\n",
    "        # recognize lack of matches\n",
    "        if last_watchpoint_match_time and (mode & MODE.TRAINING or mode & MODE.DETECTION_THRESHOLD_CALIBRATION):\n",
    "            delta = time.time() - last_watchpoint_match_time\n",
    "            anomaly_detection.update_max_interval(delta)\n",
    "\n",
    "        last_watchpoint_match_time = time.time()\n",
    "        \n",
    "        items_since_last_send += items_transferred\n",
    "        \n",
    "        # recognize too many matches (data_loss, because in that case collected items are not processed)\n",
    "        processing_limit = operational_config.get_items_collected_processing_limit()\n",
    "        if items_transferred > processing_limit:\n",
    "            print(f'items_transferred ({items_transferred}) is above limit ({processing_limit}) ignoring chunk')\n",
    "            processing_limit_exceeded = True\n",
    "            # only consider data_loss an anomaly if it occurs during testing (called \"monitoring\" in GUI)\n",
    "            if mode & MODE.TESTING:\n",
    "                # this condition prevents spamming send_new_anomaly\n",
    "                if processing_limit_exceeded_reseted:\n",
    "                    send_new_anomaly(None, None, None, None, data_loss = True)\n",
    "                    processing_limit_exceeded_reseted = False     \n",
    "            # update last match time for the sake of recognizing lack of matches later\n",
    "            continue\n",
    "        \n",
    "        processing_limit_exceeded_reseted = True\n",
    "        \n",
    "\n",
    "        \n",
    "    #     events, events_overflows, pcs, clk_counters, instrs, instr_names, instr_strings, fifo_full_ticks_counts, all_gp_regs = parse_input_buffer(input_buffer, items_transferred, dont_decode=True)\n",
    "    #     df = pd.DataFrame(zip(pcs,clk_counters,instrs,instr_names,instr_strings,fifo_full_ticks_counts), columns=['pc','clk_counter','instr', 'instr_names', 'instr_strings', 'fifo_full_ticks_counts'])\n",
    "    #     # all_gp_regs is a list of dicts, it is joined below into the main dataframe\n",
    "    #     df = df.join( pd.DataFrame.from_dict(all_gp_regs) )\n",
    "    #     df.iloc[:,0] = df.iloc[:,0].apply(lambda x: f'{x:08X}')\n",
    "    #     print( df.iloc[:items_transferred] )\n",
    "        start = 0\n",
    "        end = chunks_per_item\n",
    "        processing_time_checkpoint = time.time()\n",
    "        for i in range(items_transferred):\n",
    "            \n",
    "            fifo_item = int.from_bytes(bytes(input_buffer[start:end]), byteorder='little')            \n",
    "    #         clk_count = (fifo_item >> CLK_LOCATION) & ((1 << 64)-1)\n",
    "    #         halted_clk_count = (fifo_item >> HALTED_CLK_LOCATION) & ((1 << 64)-1)            \n",
    "            metrics_dict = parse_fifo_item(fifo_item, Packet_Format.data_pkt)\n",
    "#             perf_counters_values = list(Packet_Format.get_perf_counters_dict_from_metrics_dict(metrics_dict).values())\n",
    "            features_vector = Packet_Format.get_vector_for_anomaly_detection_from_metrics_dict(metrics_dict)\n",
    "#             features_dict = Packet_Format.get_dict_for_anomaly_detection_from_metrics_dict(metrics_dict)\n",
    "            \n",
    "            # variable to avoid calculating the same thing twice\n",
    "            if mode != MODE.IDLE:\n",
    "                similarity, most_similar_vector = anomaly_detection.get_similarity(features_vector)\n",
    "                similarities.append(similarity)\n",
    "                if operational_config.is_raw_data_send_enabled():\n",
    "                    send_raw_data({\n",
    "                        **metrics_dict, \n",
    "                        'similarity' : similarity, \n",
    "                        'is_anomaly' : bool(similarity < anomaly_detection.get_similarity_threshold()),\n",
    "                        'time' : datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "                    })\n",
    "            \n",
    "            if mode & MODE.DETECTION_THRESHOLD_CALIBRATION:\n",
    "                if similarity < anomaly_detection.get_similarity_threshold() and similarity > 0:\n",
    "                    # similarity > 0 is used because watchpoints collected at previously unseen PC\n",
    "                    # will result in 0 similarity, and it shouldn't be used as threshold\n",
    "                    anomaly_detection.set_similarity_threshold(similarity)\n",
    "                    print(f'DETECTION_THRESHOLD_CALIBRATION: Updated similarity threshold to {anomaly_detection.get_similarity_threshold()}')\n",
    "                    send_similarity_threshold(anomaly_detection.get_similarity_threshold())\n",
    "                \n",
    "            if mode & MODE.TESTING:\n",
    "                if similarity != 1:\n",
    "                    print(f'Similarity of the following performance counters were not 1 ({similarity}):')\n",
    "                    for event_name in event_names:\n",
    "                        print(f'{event_name:<13}', end='')\n",
    "                    print()\n",
    "                    for v in features_vector:\n",
    "                        print(f'{v:<13}', end='')\n",
    "                    print()\n",
    "                    if most_similar_vector is not None:\n",
    "                        print(f'Most similar vector:')\n",
    "                        for v in most_similar_vector:\n",
    "                            print(f'{v:<13}', end='')\n",
    "                    print('\\n')\n",
    "            \n",
    "                if similarity < anomaly_detection.get_similarity_threshold():\n",
    "                    send_new_anomaly(metrics_dict, similarity, features_vector, most_similar_vector)\n",
    "                        \n",
    "            if mode & MODE.TRAINING:\n",
    "                anomaly_detection.update_dataset(features_vector)\n",
    "            \n",
    "            \n",
    "            \n",
    "            total_clk_time += metrics_dict[\"clk_counter\"]\n",
    "            total_halted_time += metrics_dict[\"fifo_full_ticks_count\"]\n",
    "            \n",
    "            if mode != MODE.IDLE:\n",
    "                clk_time_since_last_send += metrics_dict[\"clk_counter\"]\n",
    "                halted_time_since_last_send += metrics_dict[\"fifo_full_ticks_count\"]\n",
    "#                 items_since_last_send += 1           \n",
    "            \n",
    "            start += chunks_per_item\n",
    "            end += chunks_per_item\n",
    "        items_transferred = 0\n",
    "        if print_stats:\n",
    "            print(f'dataset size = {anomaly_detection.get_dataset_size()}', end=', ')\n",
    "            print(f'processing time: {time.time() - processing_time_checkpoint}s')\n",
    "#         time.sleep(1)\n",
    "\n",
    "operate_thread = Thread(target=operate, daemon=True)\n",
    "operate_thread.start()\n",
    "\n",
    "#print(f'Total clk_count = {total_clk_time / CLK_SPEED}s')\n",
    "#print(f'Total halted_time = {total_halted_time / CLK_SPEED}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_stats = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# end_operate_thread = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# s = console_io.read()\n",
    "# for line in s.split('\\n'):\n",
    "#     print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for row in anomaly_detection.dataset:\n",
    "#     for val in row:\n",
    "#         print(int(val), end=', ')\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New connection at ID 0 ('192.168.0.105', 63325)\n",
      "Calling rpc_list_programs with args=[]\n",
      "Calling rpc_update_status with args=[]\n",
      "Calling rpc_set_operational_config with args=['{\"raw_data_send_enable\":true}']\n",
      "Saving config:  {'disable_saving': False, 'f_name': 'config.pickle', 'periodic_send_interval_seconds': 0.8, 'items_collected_processing_limit': 500, 'lack_of_matches_threshold_multiplier_of_max_interval': 1.5, 'raw_data_send_enable': True}\n",
      "Calling rpc_load_program with args=['dsbd']\n",
      "Calling rpc_enable_training with args=[]\n",
      "Calling rpc_enable_testing with args=[]\n",
      "Calling rpc_run with args=[]\n",
      "Similarity of the following performance counters were not 1 (0.0):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484400   24689        500          0            0            531          507          558          195          4141         558          0            201          \n",
      "\n",
      "\n",
      "Similarity of the following performance counters were not 1 (0.3084349945187086):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484400   25000179     0            0            0            641027       2            641028       0            1923103      641028       0            0            \n",
      "Most similar vector:\n",
      "2147484400.0 24689.0      500.0        0.0          0.0          531.0        507.0        558.0        195.0        4141.0       558.0        0.0          201.0        \n",
      "\n",
      "Similarity of the following performance counters were not 1 (0.9999994553905628):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484400   25000106     0            0            0            641027       2            641028       0            1923095      641028       0            0            \n",
      "Most similar vector:\n",
      "2147484400.0 25000179.0   0.0          0.0          0.0          641027.0     2.0          641028.0     0.0          1923103.0    641028.0     0.0          0.0          \n",
      "\n",
      "Calling rpc_list_available_models with args=[]\n",
      "Calling rpc_load_detection_model with args=['dsbd.npy']\n",
      "Calling rpc_update_status with args=[]\n",
      "Similarity of the following performance counters were not 1 (0.9998931883624599):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484400   15065505     500          2147572264   875770417    386069       1474         386114       66           1164335      386114       0            50           \n",
      "Most similar vector:\n",
      "2147484400.0 15065595.0   500.0        2147572264.0 875770417.0  386071.0     1476.0       386116.0     66.0         1164349.0    386116.0     0.0          50.0         \n",
      "\n",
      "bc:0165231842759\n",
      "Similarity of the following performance counters were not 1 (0.999999307800343):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484400   25101122     500          2147739168   0            643364       2197         643557       337          1940195      643557       0            320          \n",
      "Most similar vector:\n",
      "2147484400.0 25100922.0   500.0        2147739168.0 0.0          643364.0     2197.0       643557.0     337.0        1940193.0    643557.0     0.0          320.0        \n",
      "\n",
      "bc:5057753897246\n",
      "Similarity of the following performance counters were not 1 (0.9994244203733913):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484232   15005112     50577538972462147739232   0            384877       70           384828       133          1155612      384828       0            134          \n",
      "Most similar vector:\n",
      "2147484232.0 15004957.0   5057753897246.02147739232.0 0.0          384877.0     70.0         384828.0     133.0        1155601.0    384828.0     0.0          133.0        \n",
      "\n",
      "Similarity of the following performance counters were not 1 (0.9997320177842742):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484400   6315         2147744536   0            0            293          93           350          324          2213         350          0            307          \n",
      "Most similar vector:\n",
      "2147484400.0 6293.0       2147744536.0 0.0          0.0          293.0        93.0         350.0        324.0        2213.0       350.0        0.0          307.0        \n",
      "\n",
      "Similarity of the following performance counters were not 1 (0.999999709250194):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484400   25100708     500          2147739168   0            643370       2203         643562       335          1940201      643562       0            313          \n",
      "Most similar vector:\n",
      "2147484400.0 25100639.0   500.0        2147739168.0 0.0          643370.0     2203.0       643562.0     335.0        1940199.0    643562.0     0.0          313.0        \n",
      "\n",
      "Error sending data to client 0: [Errno 32] Broken pipe\n",
      "New connection at ID 1 ('192.168.0.105', 63360)\n",
      "Calling rpc_list_programs with args=[]\n",
      "Calling rpc_update_status with args=[]\n",
      "Calling rpc_set_operational_config with args=['{\"raw_data_send_enable\":true}']\n",
      "Saving config:  {'disable_saving': False, 'f_name': 'config.pickle', 'periodic_send_interval_seconds': 0.8, 'items_collected_processing_limit': 500, 'lack_of_matches_threshold_multiplier_of_max_interval': 1.5, 'raw_data_send_enable': True}\n",
      "Similarity of the following performance counters were not 1 (0.9999995696093075):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484232   15004944     40621390154052147739232   0            384878       70           384826       133          1155591      384826       0            133          \n",
      "Most similar vector:\n",
      "2147484232.0 15004899.0   4062139015405.02147739232.0 0.0          384878.0     70.0         384826.0     133.0        1155594.0    384826.0     0.0          133.0        \n",
      "\n",
      "Similarity of the following performance counters were not 1 (0.9998064530415035):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484400   6120         2147744536   0            0            293          93           350          324          2184         350          0            307          \n",
      "Most similar vector:\n",
      "2147484400.0 6127.0       2147744536.0 0.0          0.0          293.0        93.0         350.0        324.0        2181.0       350.0        0.0          307.0        \n",
      "\n",
      "bc:4062139015405\n",
      "Similarity of the following performance counters were not 1 (0.9999992495547283):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484400   25100432     500          2147739168   0            643366       2199         643558       335          1940165      643558       0            312          \n",
      "Most similar vector:\n",
      "2147484400.0 25100213.0   500.0        2147739168.0 0.0          643366.0     2199.0       643558.0     335.0        1940167.0    643558.0     0.0          312.0        \n",
      "\n",
      "Calling rpc_halt with args=[]\n",
      "continuous_monitoring_system_controller config stored:\n",
      "{'atf_active': True,\n",
      " 'atf_mode': 1,\n",
      " 'basic_trace_filter_mode': 1,\n",
      " 'basic_trace_filter_time_interval_ticks': 1000,\n",
      " 'basic_trace_filter_time_interval_type': 1,\n",
      " 'external_trace_filter_mode_enabled': False,\n",
      " 'feature_extractor_halting_cpu_enabled': False,\n",
      " 'halting_cpu': False,\n",
      " 'monitored_address_range_lower_bound': 4095,\n",
      " 'monitored_address_range_lower_bound_enabled': False,\n",
      " 'monitored_address_range_upper_bound': 2147483903,\n",
      " 'monitored_address_range_upper_bound_enabled': False,\n",
      " 'tlast_interval': 0,\n",
      " 'trigger_trace_end_address': 2147483910,\n",
      " 'trigger_trace_end_address_enabled': False,\n",
      " 'trigger_trace_start_address': 4096,\n",
      " 'trigger_trace_start_address_enabled': False}\n",
      "Error sending data to client 1: [Errno 32] Broken pipe\n",
      "New connection at ID 2 ('192.168.0.105', 63756)\n",
      "Calling rpc_list_programs with args=[]\n",
      "Calling rpc_update_status with args=[]\n",
      "Calling rpc_set_operational_config with args=['{\"raw_data_send_enable\":true}']\n",
      "Saving config:  {'disable_saving': False, 'f_name': 'config.pickle', 'periodic_send_interval_seconds': 0.8, 'items_collected_processing_limit': 500, 'lack_of_matches_threshold_multiplier_of_max_interval': 1.5, 'raw_data_send_enable': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling rpc_run with args=[]\n",
      "continuous_monitoring_system_controller config stored:\n",
      "{'atf_active': True,\n",
      " 'atf_mode': 1,\n",
      " 'basic_trace_filter_mode': 1,\n",
      " 'basic_trace_filter_time_interval_ticks': 1000,\n",
      " 'basic_trace_filter_time_interval_type': 1,\n",
      " 'external_trace_filter_mode_enabled': False,\n",
      " 'feature_extractor_halting_cpu_enabled': False,\n",
      " 'halting_cpu': False,\n",
      " 'monitored_address_range_lower_bound': 4095,\n",
      " 'monitored_address_range_lower_bound_enabled': False,\n",
      " 'monitored_address_range_upper_bound': 2147483903,\n",
      " 'monitored_address_range_upper_bound_enabled': False,\n",
      " 'tlast_interval': 0,\n",
      " 'trigger_trace_end_address': 2147483910,\n",
      " 'trigger_trace_end_address_enabled': False,\n",
      " 'trigger_trace_start_address': 4096,\n",
      " 'trigger_trace_start_address_enabled': False}\n",
      "Similarity of the following performance counters were not 1 (0.9225617450617342):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484400   6748180189   0            2147739168   0            639360       1            639361       0            1918093      639361       0            0            \n",
      "Most similar vector:\n",
      "2147484400.0 25000105.0   0.0          2147739168.0 0.0          641027.0     1.0          641028.0     0.0          1923094.0    641028.0     0.0          0.0          \n",
      "\n",
      "Similarity of the following performance counters were not 1 (0.9999997744336218):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484232   15004964     89457191751712147739232   0            384877       71           384829       133          1155604      384829       0            133          \n",
      "Most similar vector:\n",
      "2147484232.0 15004920.0   8945719175171.02147739232.0 0.0          384877.0     71.0         384829.0     133.0        1155604.0    384829.0     0.0          133.0        \n",
      "\n",
      "Similarity of the following performance counters were not 1 (0.9999651355221816):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484400   8522         2147744536   0            0            412          107          450          422          2976         450          0            405          \n",
      "Most similar vector:\n",
      "2147484400.0 8523.0       2147744536.0 0.0          0.0          412.0        107.0        450.0        422.0        2977.0       450.0        0.0          405.0        \n",
      "\n",
      "bc:8945719175171\n",
      "Similarity of the following performance counters were not 1 (0.9999998651570648):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484400   25100428     500          2147739168   0            643367       2200         643559       335          1940164      643559       0            312          \n",
      "Most similar vector:\n",
      "2147484400.0 25100384.0   500.0        2147739168.0 0.0          643367.0     2200.0       643559.0     335.0        1940164.0    643559.0     0.0          312.0        \n",
      "\n",
      "Error sending data to client 2: [Errno 32] Broken pipe\n",
      "New connection at ID 3 ('192.168.0.105', 63804)\n",
      "Calling rpc_list_programs with args=[]\n",
      "Calling rpc_update_status with args=[]\n",
      "Calling rpc_set_operational_config with args=['{\"raw_data_send_enable\":true}']\n",
      "Saving config:  {'disable_saving': False, 'f_name': 'config.pickle', 'periodic_send_interval_seconds': 0.8, 'items_collected_processing_limit': 500, 'lack_of_matches_threshold_multiplier_of_max_interval': 1.5, 'raw_data_send_enable': True}\n",
      "bc:4062139015405\n",
      "Similarity of the following performance counters were not 1 (0.9999996002088524):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484232   15004800     40621390154052147739232   0            384878       69           384826       133          1155593      384826       0            133          \n",
      "Most similar vector:\n",
      "2147484232.0 15004865.0   4062139015405.02147739232.0 0.0          384878.0     69.0         384826.0     133.0        1155592.0    384826.0     0.0          133.0        \n",
      "\n",
      "Similarity of the following performance counters were not 1 (0.9999270822169782):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484400   6124         2147744536   0            0            293          93           350          324          2182         350          0            307          \n",
      "Most similar vector:\n",
      "2147484400.0 6127.0       2147744536.0 0.0          0.0          293.0        93.0         350.0        324.0        2181.0       350.0        0.0          307.0        \n",
      "\n",
      "Similarity of the following performance counters were not 1 (0.9999999358348413):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484400   25100262     500          2147739168   0            643363       2196         643555       335          1940144      643555       0            312          \n",
      "Most similar vector:\n",
      "2147484400.0 25100270.0   500.0        2147739168.0 0.0          643363.0     2196.0       643555.0     335.0        1940145.0    643555.0     0.0          312.0        \n",
      "\n",
      "Error sending data to client 3: [Errno 32] Broken pipe\n",
      "New connection at ID 4 ('192.168.0.105', 63809)\n",
      "Calling rpc_list_programs with args=[]\n",
      "Calling rpc_update_status with args=[]\n",
      "Calling rpc_set_operational_config with args=['{\"raw_data_send_enable\":true}']\n",
      "Saving config:  {'disable_saving': False, 'f_name': 'config.pickle', 'periodic_send_interval_seconds': 0.8, 'items_collected_processing_limit': 500, 'lack_of_matches_threshold_multiplier_of_max_interval': 1.5, 'raw_data_send_enable': True}\n",
      "bc:4062139015405\n",
      "Similarity of the following performance counters were not 1 (0.9999997541641334):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484232   15005023     40621390154052147739232   0            384878       70           384826       133          1155591      384826       0            133          \n",
      "Most similar vector:\n",
      "2147484232.0 15005032.0   4062139015405.02147739232.0 0.0          384878.0     70.0         384826.0     133.0        1155594.0    384826.0     0.0          133.0        \n",
      "\n",
      "Similarity of the following performance counters were not 1 (0.9999128927663621):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484400   5928         2147744536   0            0            293          93           350          324          2182         350          0            307          \n",
      "Most similar vector:\n",
      "2147484400.0 5932.0       2147744536.0 0.0          0.0          293.0        93.0         350.0        324.0        2183.0       350.0        0.0          307.0        \n",
      "\n",
      "Similarity of the following performance counters were not 1 (0.9999996019811103):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484400   25100207     500          2147739168   0            643362       2195         643554       335          1940138      643554       0            312          \n",
      "Most similar vector:\n",
      "2147484400.0 25100103.0   500.0        2147739168.0 0.0          643362.0     2195.0       643554.0     335.0        1940140.0    643554.0     0.0          312.0        \n",
      "\n",
      "bc:0165231842759\n",
      "Similarity of the following performance counters were not 1 (0.9999996668577261):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484232   15004986     165231842759 2147739232   0            384879       72           384828       133          1155606      384828       0            133          \n",
      "Most similar vector:\n",
      "2147484232.0 15005038.0   165231842759.02147739232.0 0.0          384879.0     72.0         384828.0     133.0        1155605.0    384828.0     0.0          133.0        \n",
      "\n",
      "Similarity of the following performance counters were not 1 (0.9992757781343052):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484400   7947         500          0            0            381          103          434          419          2818         434          0            402          \n",
      "Most similar vector:\n",
      "2147484400.0 7875.0       500.0        0.0          0.0          381.0        103.0        434.0        419.0        2817.0       434.0        0.0          402.0        \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of the following performance counters were not 1 (0.999999618648901):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484400   25100095     500          2147739168   0            643362       2195         643554       335          1940131      643554       0            312          \n",
      "Most similar vector:\n",
      "2147484400.0 25100103.0   500.0        2147739168.0 0.0          643362.0     2195.0       643554.0     335.0        1940140.0    643554.0     0.0          312.0        \n",
      "\n",
      "Error sending data to client 4: [Errno 32] Broken pipe\n",
      "New connection at ID 5 ('192.168.0.105', 63824)\n",
      "Calling rpc_list_programs with args=[]\n",
      "Calling rpc_update_status with args=[]\n",
      "Calling rpc_set_operational_config with args=['{\"raw_data_send_enable\":true}']\n",
      "Saving config:  {'disable_saving': False, 'f_name': 'config.pickle', 'periodic_send_interval_seconds': 0.8, 'items_collected_processing_limit': 500, 'lack_of_matches_threshold_multiplier_of_max_interval': 1.5, 'raw_data_send_enable': True}\n",
      "bc:8945719175171\n",
      "Similarity of the following performance counters were not 1 (0.9999996566064362):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484232   15005132     89457191751712147739232   0            384877       70           384829       133          1155605      384829       0            133          \n",
      "Most similar vector:\n",
      "2147484232.0 15005078.0   8945719175171.02147739232.0 0.0          384877.0     70.0         384829.0     133.0        1155604.0    384829.0     0.0          133.0        \n",
      "\n",
      "Similarity of the following performance counters were not 1 (0.999912468904973):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484400   8727         500          0            0            412          108          450          422          2978         450          0            405          \n",
      "Most similar vector:\n",
      "2147484400.0 8720.0       500.0        0.0          0.0          412.0        108.0        450.0        422.0        2977.0       450.0        0.0          405.0        \n",
      "\n",
      "Similarity of the following performance counters were not 1 (0.9999998383428571):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484400   25100261     500          2147739168   0            643363       2196         643555       335          1940140      643555       0            312          \n",
      "Most similar vector:\n",
      "2147484400.0 25100262.0   500.0        2147739168.0 0.0          643363.0     2196.0       643555.0     335.0        1940144.0    643555.0     0.0          312.0        \n",
      "\n",
      "Client ('192.168.0.105', 63824) has disconnected\n",
      "New connection at ID 6 ('192.168.0.105', 63828)\n",
      "Calling rpc_list_programs with args=[]\n",
      "Calling rpc_update_status with args=[]\n",
      "Calling rpc_set_operational_config with args=['{\"raw_data_send_enable\":true}']\n",
      "Saving config:  {'disable_saving': False, 'f_name': 'config.pickle', 'periodic_send_interval_seconds': 0.8, 'items_collected_processing_limit': 500, 'lack_of_matches_threshold_multiplier_of_max_interval': 1.5, 'raw_data_send_enable': True}\n",
      "bc:8945719175171\n",
      "Similarity of the following performance counters were not 1 (0.9999996925688197):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484232   15004998     89457191751712147739232   0            384877       71           384829       133          1155606      384829       0            133          \n",
      "Most similar vector:\n",
      "2147484232.0 15004964.0   8945719175171.02147739232.0 0.0          384877.0     71.0         384829.0     133.0        1155604.0    384829.0     0.0          133.0        \n",
      "\n",
      "Similarity of the following performance counters were not 1 (0.9999564325782624):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484400   8678         2147744536   0            0            412          107          450          422          2977         450          0            405          \n",
      "Most similar vector:\n",
      "2147484400.0 8676.0       2147744536.0 0.0          0.0          412.0        107.0        450.0        422.0        2976.0       450.0        0.0          405.0        \n",
      "\n",
      "Similarity of the following performance counters were not 1 (0.9999996261193866):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484400   25100567     500          2147739168   0            643370       2203         643562       335          1940186      643562       0            312          \n",
      "Most similar vector:\n",
      "2147484400.0 25100445.0   500.0        2147739168.0 0.0          643370.0     2203.0       643562.0     335.0        1940186.0    643562.0     0.0          312.0        \n",
      "\n",
      "Client ('192.168.0.105', 63828) has disconnected\n",
      "New connection at ID 7 ('192.168.0.105', 63951)\n",
      "Calling rpc_list_programs with args=[]\n",
      "Calling rpc_update_status with args=[]\n",
      "Calling rpc_set_operational_config with args=['{\"raw_data_send_enable\":true}']\n",
      "Saving config:  {'disable_saving': False, 'f_name': 'config.pickle', 'periodic_send_interval_seconds': 0.8, 'items_collected_processing_limit': 500, 'lack_of_matches_threshold_multiplier_of_max_interval': 1.5, 'raw_data_send_enable': True}\n",
      "bc:0165231842759\n",
      "Similarity of the following performance counters were not 1 (0.9999998104789241):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484232   15005116     165231842759 2147739232   0            384879       72           384828       133          1155606      384828       0            133          \n",
      "Most similar vector:\n",
      "2147484232.0 15005105.0   165231842759.02147739232.0 0.0          384879.0     72.0         384828.0     133.0        1155608.0    384828.0     0.0          133.0        \n",
      "\n",
      "Similarity of the following performance counters were not 1 (0.9998757215145316):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484400   7725         2147744536   0            0            381          102          434          419          2819         434          0            402          \n",
      "Most similar vector:\n",
      "2147484400.0 7718.0       2147744536.0 0.0          0.0          381.0        102.0        434.0        419.0        2817.0       434.0        0.0          402.0        \n",
      "\n",
      "Similarity of the following performance counters were not 1 (0.9999995816863598):\n",
      "Core__BRANCH Core__JAL    Core__LOAD   Core__STORE  L1I__LD      L1D__LD      TGC__WRITE   TGC__READ    \n",
      "2147484400   25100465     500          2147739168   0            643366       2199         643558       335          1940157      643558       0            312          \n",
      "Most similar vector:\n",
      "2147484400.0 25100432.0   500.0        2147739168.0 0.0          643366.0     2199.0       643558.0     335.0        1940165.0    643558.0     0.0          312.0        \n",
      "\n",
      "Error sending data to client 7: [Errno 32] Broken pipe\n",
      "New connection at ID 8 ('192.168.0.105', 63956)\n",
      "Calling rpc_list_programs with args=[]\n",
      "Calling rpc_update_status with args=[]\n",
      "Calling rpc_set_operational_config with args=['{\"raw_data_send_enable\":true}']\n",
      "Saving config:  {'disable_saving': False, 'f_name': 'config.pickle', 'periodic_send_interval_seconds': 0.8, 'items_collected_processing_limit': 500, 'lack_of_matches_threshold_multiplier_of_max_interval': 1.5, 'raw_data_send_enable': True}\n",
      "Error sending data to client 8: [Errno 32] Broken pipe\n",
      "New connection at ID 9 ('192.168.0.105', 63958)\n",
      "Calling rpc_list_programs with args=[]\n",
      "Calling rpc_update_status with args=[]\n",
      "Calling rpc_set_operational_config with args=['{\"raw_data_send_enable\":true}']\n",
      "Saving config:  {'disable_saving': False, 'f_name': 'config.pickle', 'periodic_send_interval_seconds': 0.8, 'items_collected_processing_limit': 500, 'lack_of_matches_threshold_multiplier_of_max_interval': 1.5, 'raw_data_send_enable': True}\n",
      "Error sending data to client 9: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "# anomaly_detection.max_interval\n",
    "# anomaly_detection.get_lack_of_matches_threshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
